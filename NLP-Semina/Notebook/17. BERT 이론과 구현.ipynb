{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT(Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "## 1. BERT의 개요\n",
    "\n",
    "![](https://wikidocs.net/images/page/35594/%ED%8C%8C%EC%9D%B8%ED%8A%9C%EB%8B%9D.PNG)\n",
    "<br />\n",
    "\n",
    "BERT는 이전 챕터에서 배웠던 트랜스포머를 이용하여 구현되었으며, 위키피디아와 같이 레이블이 없는 텍스트 데이터로 훈련된 언어 모델입니다.\n",
    "<br />\n",
    "\n",
    "BERT가 높은 성능을 얻을수 있었던 것은, 레이블이 없는 방대한 데이터로 사전 훈련된 모델을 가지고, 레이블이 있는 다른 작업에서 추가 훈련과 함께 하이퍼파라미터를 재조정하여 이 모델을 사용하면 성능이 높게 나오는 기존의 사례들을 참고하였기 때문입니다. 다른 작업에 대해서 파라미터 재조정을 위한 추가 훈련 과정을 파인 튜닝(Fine-tunning)이라고 합니다.\n",
    "<br />\n",
    "\n",
    "위의 그림이 BERT의 파인 튜닝 사례입니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BERT의 크기\n",
    "\n",
    "![](https://wikidocs.net/images/page/35594/bartbase%EC%99%80large.PNG)\n",
    "<br />\n",
    "\n",
    "BERT의 기본 구조는 트랜스포머의 인코더를 쌓아올린 구조입니다. \n",
    "<br />\n",
    "\n",
    "## 3. BERT의 문맥을 반영한 임베딩(Contextual Embedding)\n",
    "\n",
    "BERT는 Contextual Embedding을 사용하고 있습니다.\n",
    "<br />\n",
    "\n",
    "BERT의 입력은 Embedding Layer를 지난 임베딩 벡터들입니다. BERT의 연산을 거친 후의 출력 임베딩은 문장의 문맥을 모두 참고한 문맥을 반영한 임베딩이 됩니다. \n",
    "\n",
    "## 4. BERT의 서브워드 토크나이저 : WordPiece\n",
    "\n",
    "BERT는 단어보다 더 작은 단위로 쪼개는 서브워드 토크나이저를 사용합니다. BERT가 사용한 토크나이저는 WordPiece 토크나이저로 서브워드 토크나이저 챕터에서 공부한 Byte Pair Encoding의 유사 알고리즘입니다. 동작 방식은 BPE와 조금 다르지만, 글자로부터 서브워드들을 병합해가는 방식으로 최종 단어 집합을 만드는 것은 BPE와 유사합니다.\n",
    "<br />\n",
    "\n",
    "서브워드 토크나이저는 기본적으로 자주 등장하는 단어는 그대로 단어 집합에 추가하지만, 자주 등장하지 않는 단어의 경우에는 더 작은 단위인 서브워드로 분리되어 서브워드들이 단어 집합에 추가된다는 아이디어를 갖고있습니다.\n",
    "<br />\n",
    "\n",
    "BERT에서 토큰화를 수행하는 방식은 다음과 같습니다.\n",
    "\n",
    "> 준비물 : 이미 훈련 데이터로부터 만들어진 단어 집합\n",
    ">\n",
    "> 1. 토큰이 단어 집합에 존재한다.  \n",
    "> => 해당 토큰을 분리하지 않는다.\n",
    ">\n",
    "> 2. 토큰이 단어 집합에 존재하지 않는다.    \n",
    "> => 해당 토큰을 서브워드로 분리한다.  \n",
    "> => 해당 토큰의 첫 번째 서브워드를 제외한 나머지 서브워드들은 앞에 '##'를 붙인 것을 토큰으로 한다.\n",
    "<br />\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Position Embedding\n",
    "\n",
    "트랜스포머에서는 Positional Encoding이라는 방법을 통해서 단어의 위치 정보를 표현했습니다. Positional Encoding은 사인 함수와 코사인 함수를 사용하여 위치에 따라 다른 값을 가지는 행렬을 만들어 이를 단어 벡터들과 더하는 방법입니다.\n",
    "<br />\n",
    "\n",
    "BERT에서는 이와 유사하지만, 위치 정보를 사인 함수와 코사인 함수로 만드는 것이 아닌 학습을 통해서 얻는 Position Embedding이라는 방법을 사용합니다.\n",
    "<br />\n",
    "\n",
    "![](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC5.PNG)\n",
    "<br />\n",
    "\n",
    "위의 그림은 포지션 임베딩을 사용하는 방법을 보여줍니다. 우선, 위의 그림에서 WordPiece Embedding은 우리가 이미 알고있는 단어 임베딩으로 실질적인 입력입니다. 그리고 이 입력에 포지션 임베딩을 통해서 위치 정보를 더해주어야 합니다. 포지션 임베딩의 아이디어는 굉장히 간단한데, 위치 정보를 위한 임베딩 층을 하나 더 사용합니다.\n",
    "<br />\n",
    "\n",
    "실제 BERT에서는 문장의 최대 길이를 512로 하고 있으므로, 총 512개의 포지션 임베딩 벡터가 학습됩니다. 결론적으로 현재 설명한 내용을 기준으로는 BERT에서 총 두 개의 임베딩 층이 사용됩니다. 단어 집합의 크기가 30,522개인 단어 벡터를 위한 임베딩층과 문장의 최대 길이가 512이므로 512개의 포지션 벡터를 위한 임베딩 층입니다.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. BERT의 사전 훈련(Pre-Training)\n",
    "\n",
    "### 1) Masked Language Model, MLM\n",
    "\n",
    "BERT는 사전 훈련을 위해서 인공 신경망의 입력으로 들어가는 입력 텍스트의 15%의 단어를 랜덤으로 마스킹(Masking)합니다. 그리고 인공 신경망에게 이 가려진 단어들을 예측하도록 합니다. \n",
    "<br />\n",
    "\n",
    "더 정확히는 전부 [MASK]로 변경하지는 않고, 랜덤으로 선택된 15%의 단어들은 다시 다음과 같은 비율로 규칙이 적용됩니다.\n",
    "<br />\n",
    "\n",
    "- 80%의 단어들은 [MASK]로 변경\n",
    "\n",
    "- 10% 단어들은 랜덤으로 다른 단어로 변경\n",
    "\n",
    "- 10%의 단어들은 변경하지 않고 유지\n",
    "<br />\n",
    "\n",
    "이렇게 하는 이유는 [MASK]만 사용할 경우에는 [MASK] 토큰이 파인 튜닝 단계에서는 나타나지 않으므로 사전 학습 단계와 파인 튜닝 단계에서의 불일치가 발생하는 문제가 있습니다. 이 문제를 완화하기 위해 위와 같은 방법을 사용하고, 그림으로 나타내면 아래와 같습니다.\n",
    "<br />\n",
    "\n",
    "![](https://wikidocs.net/images/page/115055/%EC%A0%84%EC%B2%B4%EB%8B%A8%EC%96%B4.PNG)\n",
    "<br />\n",
    "\n",
    "'My dog is cute. he likes playing'이라는 문장에 대해서 Masked Language Model을 학습하고자 합니다. 해당 데이터셋이 아래와 같이 변경되었다고 가정하고 어떻게 학습하는지를 보겠습니다.\n",
    "<br />\n",
    "\n",
    "- 'dog' -> [MASK]\n",
    "\n",
    "- 'he' -> 'king'\n",
    "\n",
    "- 'play' -> 'play'\n",
    "<br />\n",
    "\n",
    "![](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC9.PNG)\n",
    "<br />\n",
    "\n",
    "BERT는 랜덤 단어 'king'으로 변경된 토큰에 대해서도 원래 단어가 무엇인지, 변경되지 않은 단어 'play'에 대해서도 원래 단어가 무엇인지를 예측해야 합니다. BERT는 이러한 Masked Language Model외에도 다음 문장 예측이라는 또 다른 태스크를 학습합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Next Sentence Prediction, NSP\n",
    "\n",
    "BERT는 두 개의 문장을 준 후에 이 문장이 이어지는 문장인지 아닌지를 맞추는 방식으로 훈련합니다. 이를 위해서 50:50 비율로 실제 이어지는 두 개의 문장과 랜덤으로 이어붙인 두 개의 문장을 주고 훈련시킵니다. 이를 각각 Sentence A와 Sentence B라고 하였을 때, 아래 예시는 문장의 연속성을 확인한 경우와 그렇지 않은 경우를 보여줍니다.\n",
    "<br />\n",
    "\n",
    "- 이어지는 문장의 경우\n",
    "\n",
    "    - Sentence A : The man went to the store.\n",
    "\n",
    "    - Sentence B : He bought a gallon of milk.\n",
    "\n",
    "    - Label : IsNextSentence\n",
    "<br />\n",
    "\n",
    "- 이어지는 문장이 아닌 경우\n",
    "\n",
    "    - Sentence A : The man went to the store.\n",
    "\n",
    "    - Sentence B : dogs are so cute.\n",
    "\n",
    "    - Label : NotNextSentence\n",
    "<br />\n",
    "\n",
    "![](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC10.PNG)\n",
    "<br />\n",
    "\n",
    "BERT의 입력으로 넣을 때에는 [SEP]라는 특별 토큰을 사용해서 문장을 구분합니다. 첫 번째 문장의 끝에 [SEP] 토큰을 넣고, 두 번째 문장이 끝나면 역시 [SEP] 토큰을 붙여줍니다. 그리고 이 두 문장이 실제 이어지는 문장인지 아닌지를 [CLS] 토큰의 위치의 출력층에서 이진 분류 문제를 풀도록 합니다. [CLS]토큰은 BERT가 분류 문제를 풀기 위해 추가된 특별 토큰입니다. 그리고 위의 그림에서 나타난 것과 같이 Masked Language Model과 Next Sentence Prediction은 따로 학습하는 것이 아닌 loss를 합하여 학습이 동시에 이루어집니다.\n",
    "<br />\n",
    "\n",
    "BERT가 이러한 NSP Task도 학습하는 이유는 Question Answering이나 Natural Language Inference와 같이 두 문장의 관계를 이해하는 것이 중요한 Task들도 해결하고자 하기때문입니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Segment Embedding\n",
    "\n",
    "![](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC7.PNG)\n",
    "<br />\n",
    "\n",
    "BERT에서는 문장 구분을 위해 Segment Embedding이라는 또 다른 임베딩 층을 사용합니다. 첫 번째 문장에는 Sentence 0 임베딩, 두 번째 문장에는 Sentence 1 임베딩을 더해주는 방식이며 임베딩 벡터는 두 개만 사용됩니다.\n",
    "<br />\n",
    "\n",
    "결론적으로 BERT는 총 3개의 임베딩 층이 사용됩니다.\n",
    "<br />\n",
    "\n",
    "- WordPiece Embedding : 실질적인 입력이 되는 워드 임베딩\n",
    "\n",
    "- Position Embedding : 위치 정보를 학습하기 위한 임베딩\n",
    "\n",
    "- Segment Embedding : 두 개의 문장을 구분하기 위한 임베딩"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Fine-tuning BERT\n",
    "\n",
    "### 1) Single Text Classification\n",
    "\n",
    "![](https://wikidocs.net/images/page/115055/apply1.PNG)\n",
    "<br />\n",
    "\n",
    "입력된 문서에 대해서 분류를 하는 유형으로 문서의 시작에 [CLS]토큰을 입력합니다. 분류 문제를 풀기 위해서 [CLS]토큰 위치의 출력층에서 Dense layer를 추가하여 분류를 하게 됩니다.\n",
    "<br />\n",
    "\n",
    "### 2) Tagging\n",
    "\n",
    "![](https://wikidocs.net/images/page/115055/apply2.PNG)\n",
    "<br />\n",
    "\n",
    "입력 텍스트의 각 토큰의 위치에 Dense layer를 사용하여 예측\n",
    "<br />\n",
    "\n",
    "### 3) Text Pair Classification or Regression\n",
    "\n",
    "![](https://wikidocs.net/images/page/115055/apply3.PNG)\n",
    "<br />\n",
    "\n",
    "대표적으로 자연어 추론(Natural Language Inference)이 있습니다. 자연어 추론 문제란, 두 문장이 주어졌을 때, 하나의 문장이 다른 문장과 논리적으로 어떤 관계에 있는지를 분류하는 것입니다. 유형으로는 모순(contradiction), 함의(entailment), 중립(neutral)이 있습니다.\n",
    "<br />\n",
    "\n",
    "### 4) Question Answering\n",
    "\n",
    "![](https://wikidocs.net/images/page/115055/apply4.PNG)\n",
    "<br />\n",
    "\n",
    "질문과 본문을 입력받으면, 본문의 일부분을 추출해서 질문에 답변하는 것입니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import collections\n",
    "import json\n",
    "import shutil\n",
    "import zipfile\n",
    "import copy\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.unicode_minus']=False\n",
    "\n",
    "import sentencepiece as spm\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 몇 가지 유틸리티 함수 정의\n",
    "def get_pad_mask(tokens, i_pad= 0):\n",
    "    \"\"\"\n",
    "    pad mask 계산\n",
    "    param tokens : tokens(bs, n_seq)\n",
    "    param i_pad : id of pad\n",
    "    return mask : pad mask (pad:1, other:0)\n",
    "    \"\"\"\n",
    "\n",
    "    mask = tf.cast(tf.math.equal(tokens, i_pad), tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=1)\n",
    "    return mask\n",
    "\n",
    "def get_ahead_mask(tokens, i_pad=0):\n",
    "    n_seq = tf.shape(tokens)[1]\n",
    "    ahead_mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)\n",
    "    ahead_mask = tf.expand_dims(ahead_mask, axis=0)\n",
    "    pad_mask = get_pad_mask(tokens, i_pad)\n",
    "    mask = tf.maximum(ahead_mask, pad_mask)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GELU\n",
    "@tf.function(experimental_relax_shapes=True)\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + tf.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_initializer(stddev=0.02):\n",
    "    return tf.keras.initializers.TruncatedNormal(stddev=stddev)\n",
    "\n",
    "def bias_initializer():\n",
    "    return tf.zeros_initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    \"\"\"\n",
    "    json을 config 형태로 사용하기 위한 Class\n",
    "    param dict : config dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        \"\"\"\n",
    "        file에서 Config를 생성함\n",
    "        param file : filename\n",
    "        \"\"\"\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, name='weight_shared_embedding'):\n",
    "        super().__init__(name=name)\n",
    "        self.n_vocab = config.n_vocab\n",
    "        self.d_model = config.d_model\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        with tf.name_scope(\"shared_embedding_weight\"):\n",
    "            self.shared_weights = self.add_weight('weights', shape=[self.n_vocab, self.d_model], initializer=kernel_initializer())\n",
    "    \n",
    "    def call(self, inputs, mode='embedding'):\n",
    "        if mode == 'embedding':\n",
    "            return self._embedding(inputs)\n",
    "        elif mode == 'linear':\n",
    "            return self._linear(inputs)\n",
    "        else:\n",
    "            raise ValueError(f\"mode {mode} is not valid\")\n",
    "    \n",
    "    def _embedding(self, inputs):\n",
    "        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))\n",
    "        return embed\n",
    "    \n",
    "    def _linear(self, inputs):\n",
    "        n_batch = tf.shape(inputs)[0]\n",
    "        n_seq = tf.shape(inputs)[1]\n",
    "\n",
    "        inputs = tf.reshape(inputs, [-1, self.d_model]) # (batch_size * n_seq, d_model)\n",
    "\n",
    "        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n",
    "        outputs = tf.reshape(outputs, [n_batch, n_seq, self.n_vocab])\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, name='position_embedding'):\n",
    "        super().__init__(name=name)\n",
    "        self.embedding = tf.keras.layers.Embedding(config.n_seq, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        position = tf.cast(tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True), tf.int32)\n",
    "        embed = self.embedding(position)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, name='scaled_dot_product_attention'):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        attn_score = tf.matmul(Q, K, transpose_b=True)\n",
    "        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n",
    "        attn_scale = tf.math.divide(attn_score, scale)\n",
    "        attn_scale -= 1e9 * attn_mask\n",
    "        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n",
    "        attn_out = tf.matmul(attn_prob, V)\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, config, name='multi_head_attention'):\n",
    "\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.d_head = config.d_head\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.wk = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.wv = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(name='self_attention')\n",
    "\n",
    "        self.wo = tf.keras.layers.Dense(config.n_head , kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "    \n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        qm = tf.transpose(tf.reshape(self.wq(Q), [batch_size, -1, self.n_head, self.d_head]), [0,2,1,3])\n",
    "        km = tf.transpose(tf.reshape(self.wk(K), [batch_size, -1, self.n_head, self.d_head]), [0,2,1,3])\n",
    "        vm = tf.transpose(tf.reshape(self.wv(V), [batch_size, -1, self.n_head, self.d_head]), [0,2,1,3])\n",
    "        attn_mask_m = tf.expand_dims(attn_mask, axis=1)\n",
    "\n",
    "        attn_out = self.attention(qm, km, vm, attn_mask_m)\n",
    "\n",
    "        attn_out_m = tf.transpose(attn_out, perm=[0,2,1,3])\n",
    "        attn_out = tf.reshape(attn_out_m, [batch_size, -1, self.n_head * self.d_head])\n",
    "        attn_out = self.wo(attn_out)\n",
    "\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, config, name='feed_forward'):\n",
    "\n",
    "        super().__init__(name=name)\n",
    "        self.w1 = tf.keras.layers.Dense(config.d_ff, activation=gelu, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.w2 = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        ff_val = self.w2(self.w1(inputs))\n",
    "        return ff_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, name='encoder_layer'):\n",
    "        super().__init__(name=name)\n",
    "        self.self_attention = MultiHeadAttention(config)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "        \n",
    "        self.ffn = PositionWiseFeedForward(config)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "    def call(self, enc_embed, self_mask):\n",
    "        self_attn_val = self.self_attention(enc_embed, enc_embed, enc_embed, self_mask)\n",
    "        norm1_val = self.norm1(enc_embed + self.dropout(self_attn_val))\n",
    "\n",
    "        ffn_val = self.ffn(norm1_val)\n",
    "        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))\n",
    "\n",
    "        return enc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, name='bert'):\n",
    "\n",
    "        super().__init__(name=name)\n",
    "        self.i_pad = config.i_pad\n",
    "        self.embedding = SharedEmbedding(config)\n",
    "        self.position = PositionEmbedding(config)\n",
    "        self.segment = tf.keras.layers.Embedding(2, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.encoder_layers = [EncoderLayer(config, name=f\"encoder_layer_{i}\") for i in range(config.n_layer)]\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        enc_tokens, segments = inputs\n",
    "\n",
    "        enc_self_mask = tf.keras.layers.Lambda(get_pad_mask, output_shape=(1, None), name='enc_self_mask')(enc_tokens, self.i_pad)\n",
    "\n",
    "        enc_embed = self.get_embedding(enc_tokens, segments)\n",
    "\n",
    "        enc_out = self.dropout(enc_embed)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            enc_out = encoder_layer(enc_out, enc_self_mask)\n",
    "\n",
    "        logits_cls = enc_out[:, 0]\n",
    "        logits_lm = self.embedding(enc_out, mode='linear')\n",
    "        return logits_cls, logits_lm\n",
    "    \n",
    "    def get_embedding(self, tokens, segments):\n",
    "        embed = self.embedding(tokens) + self.position(tokens) + self.segment(segments)\n",
    "        embed = self.norm(embed)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PooledOutput(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, n_output, name=\"pooled_output\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.dense1 = tf.keras.layers.Dense(config.d_model, activation=tf.nn.tanh, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.dense2 = tf.keras.layers.Dense(n_output, use_bias=False, activation=tf.nn.softmax, name=\"nsp\", kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    " \n",
    "    def call(self, inputs):\n",
    "        outputs = self.dense1(inputs)\n",
    "        outputs = self.dense2(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_pre_train(config):\n",
    "    enc_tokens = tf.keras.layers.Input((None,), name=\"enc_tokens\")\n",
    "    segments = tf.keras.layers.Input((None,), name=\"segments\")\n",
    "\n",
    "    bert = BERT(config)\n",
    "\n",
    "    logits_cls, logits_lm = bert((enc_tokens, segments))\n",
    "\n",
    "    # [CLS]토큰의 마지막 hidden state를 이용해서 IsNext 인지 NotNext인지를 판단함.\n",
    "    logits_cls = PooledOutput(config, 2, name=\"pooled_nsp\")(logits_cls)\n",
    "    outputs_nsp = tf.keras.layers.Softmax(name=\"nsp\")(logits_cls)\n",
    "\n",
    "    # masked token 예측\n",
    "    outputs_mlm = tf.keras.layers.Softmax(name=\"mlm\")(logits_lm)\n",
    "\n",
    "    model = tf.keras.Model(inputs=(enc_tokens, segments), outputs=(outputs_nsp, outputs_mlm))\n",
    "    return model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "96512990021685504d4683198faad895f5cd0e4b7b1aa29365fef97d0a48eb34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
