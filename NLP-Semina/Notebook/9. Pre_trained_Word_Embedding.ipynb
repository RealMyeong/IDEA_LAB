{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사전학습 임베딩 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sentences = ['nice great best amazing', 'stop lies', 'pitiful nerd', 'excellent work', 'supreme quality', 'bad', 'highly respectable']\n",
    "y_train = [1, 0, 0, 1, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'nice': 1,\n",
       " 'great': 2,\n",
       " 'best': 3,\n",
       " 'amazing': 4,\n",
       " 'stop': 5,\n",
       " 'lies': 6,\n",
       " 'pitiful': 7,\n",
       " 'nerd': 8,\n",
       " 'excellent': 9,\n",
       " 'work': 10,\n",
       " 'supreme': 11,\n",
       " 'quality': 12,\n",
       " 'bad': 13,\n",
       " 'highly': 14,\n",
       " 'respectable': 15}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13], [14, 15]]\n"
     ]
    }
   ],
   "source": [
    "x_encoded = tokenizer.texts_to_sequences(sentences)\n",
    "print(x_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4]\n",
      " [ 5  6  0  0]\n",
      " [ 7  8  0  0]\n",
      " [ 9 10  0  0]\n",
      " [11 12  0  0]\n",
      " [13  0  0  0]\n",
      " [14 15  0  0]]\n"
     ]
    }
   ],
   "source": [
    "x_train = pad_sequences(x_encoded, maxlen=4, padding='post')\n",
    "y_train = np.array(y_train)\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 - 0s - loss: 0.6919 - acc: 0.5714 - 328ms/epoch - 328ms/step\n",
      "Epoch 2/10\n",
      "1/1 - 0s - loss: 0.6904 - acc: 0.5714 - 5ms/epoch - 5ms/step\n",
      "Epoch 3/10\n",
      "1/1 - 0s - loss: 0.6889 - acc: 0.5714 - 5ms/epoch - 5ms/step\n",
      "Epoch 4/10\n",
      "1/1 - 0s - loss: 0.6875 - acc: 0.5714 - 9ms/epoch - 9ms/step\n",
      "Epoch 5/10\n",
      "1/1 - 0s - loss: 0.6860 - acc: 0.5714 - 5ms/epoch - 5ms/step\n",
      "Epoch 6/10\n",
      "1/1 - 0s - loss: 0.6846 - acc: 0.5714 - 12ms/epoch - 12ms/step\n",
      "Epoch 7/10\n",
      "1/1 - 0s - loss: 0.6831 - acc: 0.7143 - 12ms/epoch - 12ms/step\n",
      "Epoch 8/10\n",
      "1/1 - 0s - loss: 0.6816 - acc: 0.7143 - 12ms/epoch - 12ms/step\n",
      "Epoch 9/10\n",
      "1/1 - 0s - loss: 0.6802 - acc: 0.8571 - 12ms/epoch - 12ms/step\n",
      "Epoch 10/10\n",
      "1/1 - 0s - loss: 0.6787 - acc: 0.8571 - 13ms/epoch - 13ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2636b730e50>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "\n",
    "embedding_dim = 4\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.fit(x_train, y_train, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 4, 4)              64        \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 81\n",
      "Trainable params: 81\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사전 학습 임베딩\n",
    "- GloVe 다운로드 : http://nlp.stanford.edu/data/glove.6B.zip\n",
    "- Word2Vec 다운로드 : https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM  \n",
    "\n",
    "1) GloVe 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve, urlopen\n",
    "import gzip\n",
    "import zipfile\n",
    "\n",
    "urlretrieve('http://nlp.stanford.edu/data/glove.6B.zip', filename='glove.6B.zip')\n",
    "zf = zipfile.ZipFile('glove.6B.zip')\n",
    "zf.extractall()\n",
    "zf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n"
     ]
    }
   ],
   "source": [
    "embedding_dict = dict()\n",
    "f = open('glove.6B.100d.txt', encoding='utf8')\n",
    "\n",
    "for line in f:\n",
    "    word_vector = line.split()\n",
    "    word = word_vector[0]\n",
    "\n",
    "    word_vector_arr = np.asarray(word_vector[1:], dtype='float32')\n",
    "    embedding_dict[word] = word_vector_arr\n",
    "f.close()\n",
    "\n",
    "print(len(embedding_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "np.shape(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('nice', 1), ('great', 2), ('best', 3), ('amazing', 4), ('stop', 5), ('lies', 6), ('pitiful', 7), ('nerd', 8), ('excellent', 9), ('work', 10), ('supreme', 11), ('quality', 12), ('bad', 13), ('highly', 14), ('respectable', 15)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.013786 ,  0.38216  ,  0.53236  ,  0.15261  , -0.29694  ,\n",
       "       -0.20558  , -0.41846  , -0.58437  , -0.77355  , -0.87866  ,\n",
       "       -0.37858  , -0.18516  , -0.128    , -0.20584  , -0.22925  ,\n",
       "       -0.42599  ,  0.3725   ,  0.26077  , -1.0702   ,  0.62916  ,\n",
       "       -0.091469 ,  0.70348  , -0.4973   , -0.77691  ,  0.66045  ,\n",
       "        0.09465  , -0.44893  ,  0.018917 ,  0.33146  , -0.35022  ,\n",
       "       -0.35789  ,  0.030313 ,  0.22253  , -0.23236  , -0.19719  ,\n",
       "       -0.0053125, -0.25848  ,  0.58081  , -0.10705  , -0.17845  ,\n",
       "       -0.16206  ,  0.087086 ,  0.63029  , -0.76649  ,  0.51619  ,\n",
       "        0.14073  ,  1.019    , -0.43136  ,  0.46138  , -0.43585  ,\n",
       "       -0.47568  ,  0.19226  ,  0.36065  ,  0.78987  ,  0.088945 ,\n",
       "       -2.7814   , -0.15366  ,  0.01015  ,  1.1798   ,  0.15168  ,\n",
       "       -0.050112 ,  1.2626   , -0.77527  ,  0.36031  ,  0.95761  ,\n",
       "       -0.11385  ,  0.28035  , -0.02591  ,  0.31246  , -0.15424  ,\n",
       "        0.3778   , -0.13599  ,  0.2946   , -0.31579  ,  0.42943  ,\n",
       "        0.086969 ,  0.019169 , -0.27242  , -0.31696  ,  0.37327  ,\n",
       "        0.61997  ,  0.13889  ,  0.17188  ,  0.30363  , -1.2776   ,\n",
       "        0.044423 , -0.52736  , -0.88536  , -0.19428  , -0.61947  ,\n",
       "       -0.10146  , -0.26301  , -0.061707 ,  0.36627  , -0.95223  ,\n",
       "       -0.39346  , -0.69183  , -1.0426   ,  0.28855  ,  0.63056  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dict['great']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 집합의 모든 단어에 대해서 사전 훈련된 GloVe 의 임베딩 벡터를 매핑\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    vector_value = embedding_dict.get(word)\n",
    "    if vector_value is not None:\n",
    "        embedding_matrix[index] = vector_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.013786  ,  0.38216001,  0.53236002,  0.15261   , -0.29694   ,\n",
       "       -0.20558   , -0.41846001, -0.58437002, -0.77354997, -0.87866002,\n",
       "       -0.37858   , -0.18516   , -0.12800001, -0.20584001, -0.22925   ,\n",
       "       -0.42598999,  0.3725    ,  0.26076999, -1.07019997,  0.62915999,\n",
       "       -0.091469  ,  0.70348001, -0.4973    , -0.77691001,  0.66044998,\n",
       "        0.09465   , -0.44893   ,  0.018917  ,  0.33146   , -0.35021999,\n",
       "       -0.35789001,  0.030313  ,  0.22253001, -0.23236001, -0.19719   ,\n",
       "       -0.0053125 , -0.25848001,  0.58081001, -0.10705   , -0.17845   ,\n",
       "       -0.16205999,  0.087086  ,  0.63028997, -0.76648998,  0.51618999,\n",
       "        0.14072999,  1.01900005, -0.43136001,  0.46138   , -0.43584999,\n",
       "       -0.47567999,  0.19226   ,  0.36065   ,  0.78987002,  0.088945  ,\n",
       "       -2.78139997, -0.15366   ,  0.01015   ,  1.17980003,  0.15167999,\n",
       "       -0.050112  ,  1.26259995, -0.77526999,  0.36030999,  0.95761001,\n",
       "       -0.11385   ,  0.28035   , -0.02591   ,  0.31246001, -0.15424   ,\n",
       "        0.37779999, -0.13598999,  0.29460001, -0.31579   ,  0.42943001,\n",
       "        0.086969  ,  0.019169  , -0.27241999, -0.31696001,  0.37327   ,\n",
       "        0.61997002,  0.13889   ,  0.17188001,  0.30362999, -1.27760005,\n",
       "        0.044423  , -0.52736002, -0.88536   , -0.19428   , -0.61947   ,\n",
       "       -0.10146   , -0.26301   , -0.061707  ,  0.36627001, -0.95222998,\n",
       "       -0.39346001, -0.69182998, -1.04260004,  0.28854999,  0.63055998])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 - 0s - loss: 0.7040 - acc: 0.5714 - 274ms/epoch - 274ms/step\n",
      "Epoch 2/10\n",
      "1/1 - 0s - loss: 0.6829 - acc: 0.5714 - 4ms/epoch - 4ms/step\n",
      "Epoch 3/10\n",
      "1/1 - 0s - loss: 0.6625 - acc: 0.5714 - 9ms/epoch - 9ms/step\n",
      "Epoch 4/10\n",
      "1/1 - 0s - loss: 0.6429 - acc: 0.7143 - 4ms/epoch - 4ms/step\n",
      "Epoch 5/10\n",
      "1/1 - 0s - loss: 0.6241 - acc: 0.7143 - 4ms/epoch - 4ms/step\n",
      "Epoch 6/10\n",
      "1/1 - 0s - loss: 0.6060 - acc: 0.7143 - 8ms/epoch - 8ms/step\n",
      "Epoch 7/10\n",
      "1/1 - 0s - loss: 0.5886 - acc: 0.7143 - 4ms/epoch - 4ms/step\n",
      "Epoch 8/10\n",
      "1/1 - 0s - loss: 0.5719 - acc: 0.7143 - 12ms/epoch - 12ms/step\n",
      "Epoch 9/10\n",
      "1/1 - 0s - loss: 0.5560 - acc: 0.7143 - 4ms/epoch - 4ms/step\n",
      "Epoch 10/10\n",
      "1/1 - 0s - loss: 0.5406 - acc: 0.7143 - 4ms/epoch - 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2665cd2cdf0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사전 학습된 임베딩 사용하여 학습하기\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "\n",
    "# 사전 훈련된 임베딩을 100차원의 값인 것으로 사용하기 때문에 맞춰줌\n",
    "output_dim=100\n",
    "\n",
    "model = Sequential()\n",
    "# 사전 훈련 임베딩을 그대로 사용하고 추가 훈련을 하지 않을 경우\n",
    "# trainable = False로 주면 됨\n",
    "e = Embedding(vocab_size, output_dim, weights=[embedding_matrix], input_length=4, trainable=False)\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.fit(x_train, y_train, epochs=10, verbose=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 사전 훈련 Word2Vec 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델의 크기(shape) : (3000000, 300)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from urllib.request import urlretrieve, urlopen\n",
    "import gzip\n",
    "import zipfile\n",
    "\n",
    "\n",
    "data_path = 'C:/Users/Myeong/dding/data/GoogleNews-vectors-negative300.bin'\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(data_path, binary=True)\n",
    "\n",
    "print('모델의 크기(shape) :',word2vec_model.vectors.shape) # 모델의 크기 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 300)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vocab_size = 16\n",
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "print(np.shape(embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(word):\n",
    "    if word in word2vec_model:\n",
    "        return word2vec_model[word]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    # 단어와 매핑되는 사전 훈련된 임베딩 벡터값\n",
    "    vector_value = get_vector(word)\n",
    "    if vector_value is not None:\n",
    "        embedding_matrix[index] = vector_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.15820312,  0.10595703, -0.18945312,  0.38671875,  0.08349609,\n",
       "       -0.26757812,  0.08349609,  0.11328125, -0.10400391,  0.17871094,\n",
       "       -0.12353516, -0.22265625, -0.01806641, -0.25390625,  0.13183594,\n",
       "        0.0859375 ,  0.16113281,  0.11083984, -0.11083984, -0.0859375 ,\n",
       "        0.0267334 ,  0.34570312,  0.15136719, -0.00415039,  0.10498047,\n",
       "        0.04907227, -0.06982422,  0.08642578,  0.03198242, -0.02844238,\n",
       "       -0.15722656,  0.11865234,  0.36132812,  0.00173187,  0.05297852,\n",
       "       -0.234375  ,  0.11767578,  0.08642578, -0.01123047,  0.25976562,\n",
       "        0.28515625, -0.11669922,  0.38476562,  0.07275391,  0.01147461,\n",
       "        0.03466797,  0.18164062, -0.03955078,  0.04199219,  0.01013184,\n",
       "       -0.06054688,  0.09765625,  0.06689453,  0.14648438, -0.12011719,\n",
       "        0.08447266, -0.06152344,  0.06347656,  0.3046875 , -0.35546875,\n",
       "       -0.2890625 ,  0.19628906, -0.33203125, -0.07128906,  0.12792969,\n",
       "        0.09619141, -0.12158203, -0.08691406, -0.12890625,  0.27734375,\n",
       "        0.265625  ,  0.1796875 ,  0.12695312,  0.06298828, -0.34375   ,\n",
       "       -0.05908203,  0.0456543 ,  0.171875  ,  0.08935547,  0.14648438,\n",
       "       -0.04638672, -0.00842285, -0.0279541 ,  0.234375  , -0.07470703,\n",
       "       -0.13574219,  0.00378418,  0.19433594,  0.05664062, -0.05419922,\n",
       "        0.06176758,  0.14160156, -0.24121094,  0.02539062, -0.15917969,\n",
       "       -0.10595703,  0.11865234,  0.24707031, -0.13574219, -0.20410156,\n",
       "       -0.30078125,  0.07910156, -0.04394531,  0.02026367, -0.05786133,\n",
       "        0.2109375 ,  0.13574219,  0.08349609, -0.0098877 , -0.10546875,\n",
       "       -0.08105469,  0.03735352, -0.10351562, -0.10205078,  0.23925781,\n",
       "       -0.21875   ,  0.05151367,  0.06738281,  0.07617188,  0.04638672,\n",
       "        0.03198242, -0.07275391,  0.14550781,  0.04858398, -0.05664062,\n",
       "       -0.07470703, -0.0030365 , -0.09277344, -0.11083984, -0.03320312,\n",
       "       -0.15234375, -0.12207031,  0.09814453,  0.375     ,  0.00454712,\n",
       "       -0.10009766,  0.02734375,  0.30078125, -0.0390625 ,  0.30078125,\n",
       "       -0.04541016, -0.00424194,  0.13671875, -0.18945312, -0.21777344,\n",
       "        0.12695312, -0.02746582, -0.18164062,  0.08984375, -0.23339844,\n",
       "        0.203125  ,  0.2734375 , -0.26953125,  0.15332031, -0.20703125,\n",
       "       -0.01153564,  0.12451172,  0.05395508, -0.23535156, -0.01409912,\n",
       "       -0.09765625,  0.20800781,  0.19335938,  0.14746094,  0.28710938,\n",
       "       -0.23046875,  0.01965332, -0.09619141, -0.0703125 , -0.04174805,\n",
       "       -0.17578125,  0.0007019 ,  0.10546875,  0.10351562,  0.02478027,\n",
       "        0.35742188,  0.17382812, -0.09570312, -0.18359375,  0.23242188,\n",
       "       -0.14453125, -0.20410156, -0.01867676,  0.06640625, -0.2265625 ,\n",
       "       -0.00582886, -0.08642578,  0.02416992, -0.07324219, -0.29882812,\n",
       "       -0.15625   ,  0.07666016,  0.19628906, -0.20410156,  0.09863281,\n",
       "       -0.01672363, -0.18652344, -0.12353516, -0.16015625, -0.10058594,\n",
       "        0.21777344,  0.09375   , -0.10058594, -0.03637695,  0.15136719,\n",
       "       -0.02526855, -0.23730469,  0.03417969, -0.00604248,  0.15625   ,\n",
       "       -0.14257812,  0.18066406, -0.35351562,  0.25      ,  0.13085938,\n",
       "       -0.04296875,  0.17089844,  0.20507812,  0.00680542, -0.08251953,\n",
       "       -0.06738281,  0.22167969, -0.16308594, -0.16699219, -0.02087402,\n",
       "        0.11035156,  0.06054688, -0.04223633, -0.17285156,  0.05029297,\n",
       "       -0.19824219,  0.01495361,  0.06542969,  0.03271484,  0.14453125,\n",
       "       -0.08691406, -0.11035156, -0.1484375 ,  0.09667969,  0.22363281,\n",
       "        0.23535156,  0.08398438,  0.18164062, -0.10595703, -0.04296875,\n",
       "        0.11572266, -0.00153351,  0.0534668 , -0.1328125 , -0.33203125,\n",
       "       -0.08251953,  0.30664062,  0.22363281,  0.27929688,  0.09082031,\n",
       "       -0.18066406, -0.00613403, -0.09423828, -0.21289062,  0.01965332,\n",
       "       -0.08105469, -0.06689453, -0.31835938, -0.08447266,  0.13574219,\n",
       "        0.0625    ,  0.07080078, -0.14257812, -0.11279297,  0.01452637,\n",
       "       -0.06689453,  0.03881836,  0.19433594,  0.09521484,  0.11376953,\n",
       "       -0.12451172,  0.13769531, -0.18847656, -0.05224609,  0.15820312,\n",
       "        0.09863281, -0.04370117, -0.06054688,  0.21679688,  0.04077148,\n",
       "       -0.14648438, -0.18945312, -0.25195312, -0.16894531, -0.08642578,\n",
       "       -0.08544922,  0.18945312, -0.14648438,  0.13476562, -0.04077148,\n",
       "        0.03271484,  0.08935547, -0.26757812,  0.00836182, -0.21386719],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model['nice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['nice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.15820312,  0.10595703, -0.18945312,  0.38671875,  0.08349609,\n",
       "       -0.26757812,  0.08349609,  0.11328125, -0.10400391,  0.17871094,\n",
       "       -0.12353516, -0.22265625, -0.01806641, -0.25390625,  0.13183594,\n",
       "        0.0859375 ,  0.16113281,  0.11083984, -0.11083984, -0.0859375 ,\n",
       "        0.0267334 ,  0.34570312,  0.15136719, -0.00415039,  0.10498047,\n",
       "        0.04907227, -0.06982422,  0.08642578,  0.03198242, -0.02844238,\n",
       "       -0.15722656,  0.11865234,  0.36132812,  0.00173187,  0.05297852,\n",
       "       -0.234375  ,  0.11767578,  0.08642578, -0.01123047,  0.25976562,\n",
       "        0.28515625, -0.11669922,  0.38476562,  0.07275391,  0.01147461,\n",
       "        0.03466797,  0.18164062, -0.03955078,  0.04199219,  0.01013184,\n",
       "       -0.06054688,  0.09765625,  0.06689453,  0.14648438, -0.12011719,\n",
       "        0.08447266, -0.06152344,  0.06347656,  0.3046875 , -0.35546875,\n",
       "       -0.2890625 ,  0.19628906, -0.33203125, -0.07128906,  0.12792969,\n",
       "        0.09619141, -0.12158203, -0.08691406, -0.12890625,  0.27734375,\n",
       "        0.265625  ,  0.1796875 ,  0.12695312,  0.06298828, -0.34375   ,\n",
       "       -0.05908203,  0.0456543 ,  0.171875  ,  0.08935547,  0.14648438,\n",
       "       -0.04638672, -0.00842285, -0.0279541 ,  0.234375  , -0.07470703,\n",
       "       -0.13574219,  0.00378418,  0.19433594,  0.05664062, -0.05419922,\n",
       "        0.06176758,  0.14160156, -0.24121094,  0.02539062, -0.15917969,\n",
       "       -0.10595703,  0.11865234,  0.24707031, -0.13574219, -0.20410156,\n",
       "       -0.30078125,  0.07910156, -0.04394531,  0.02026367, -0.05786133,\n",
       "        0.2109375 ,  0.13574219,  0.08349609, -0.0098877 , -0.10546875,\n",
       "       -0.08105469,  0.03735352, -0.10351562, -0.10205078,  0.23925781,\n",
       "       -0.21875   ,  0.05151367,  0.06738281,  0.07617188,  0.04638672,\n",
       "        0.03198242, -0.07275391,  0.14550781,  0.04858398, -0.05664062,\n",
       "       -0.07470703, -0.0030365 , -0.09277344, -0.11083984, -0.03320312,\n",
       "       -0.15234375, -0.12207031,  0.09814453,  0.375     ,  0.00454712,\n",
       "       -0.10009766,  0.02734375,  0.30078125, -0.0390625 ,  0.30078125,\n",
       "       -0.04541016, -0.00424194,  0.13671875, -0.18945312, -0.21777344,\n",
       "        0.12695312, -0.02746582, -0.18164062,  0.08984375, -0.23339844,\n",
       "        0.203125  ,  0.2734375 , -0.26953125,  0.15332031, -0.20703125,\n",
       "       -0.01153564,  0.12451172,  0.05395508, -0.23535156, -0.01409912,\n",
       "       -0.09765625,  0.20800781,  0.19335938,  0.14746094,  0.28710938,\n",
       "       -0.23046875,  0.01965332, -0.09619141, -0.0703125 , -0.04174805,\n",
       "       -0.17578125,  0.0007019 ,  0.10546875,  0.10351562,  0.02478027,\n",
       "        0.35742188,  0.17382812, -0.09570312, -0.18359375,  0.23242188,\n",
       "       -0.14453125, -0.20410156, -0.01867676,  0.06640625, -0.2265625 ,\n",
       "       -0.00582886, -0.08642578,  0.02416992, -0.07324219, -0.29882812,\n",
       "       -0.15625   ,  0.07666016,  0.19628906, -0.20410156,  0.09863281,\n",
       "       -0.01672363, -0.18652344, -0.12353516, -0.16015625, -0.10058594,\n",
       "        0.21777344,  0.09375   , -0.10058594, -0.03637695,  0.15136719,\n",
       "       -0.02526855, -0.23730469,  0.03417969, -0.00604248,  0.15625   ,\n",
       "       -0.14257812,  0.18066406, -0.35351562,  0.25      ,  0.13085938,\n",
       "       -0.04296875,  0.17089844,  0.20507812,  0.00680542, -0.08251953,\n",
       "       -0.06738281,  0.22167969, -0.16308594, -0.16699219, -0.02087402,\n",
       "        0.11035156,  0.06054688, -0.04223633, -0.17285156,  0.05029297,\n",
       "       -0.19824219,  0.01495361,  0.06542969,  0.03271484,  0.14453125,\n",
       "       -0.08691406, -0.11035156, -0.1484375 ,  0.09667969,  0.22363281,\n",
       "        0.23535156,  0.08398438,  0.18164062, -0.10595703, -0.04296875,\n",
       "        0.11572266, -0.00153351,  0.0534668 , -0.1328125 , -0.33203125,\n",
       "       -0.08251953,  0.30664062,  0.22363281,  0.27929688,  0.09082031,\n",
       "       -0.18066406, -0.00613403, -0.09423828, -0.21289062,  0.01965332,\n",
       "       -0.08105469, -0.06689453, -0.31835938, -0.08447266,  0.13574219,\n",
       "        0.0625    ,  0.07080078, -0.14257812, -0.11279297,  0.01452637,\n",
       "       -0.06689453,  0.03881836,  0.19433594,  0.09521484,  0.11376953,\n",
       "       -0.12451172,  0.13769531, -0.18847656, -0.05224609,  0.15820312,\n",
       "        0.09863281, -0.04370117, -0.06054688,  0.21679688,  0.04077148,\n",
       "       -0.14648438, -0.18945312, -0.25195312, -0.16894531, -0.08642578,\n",
       "       -0.08544922,  0.18945312, -0.14648438,  0.13476562, -0.04077148,\n",
       "        0.03271484,  0.08935547, -0.26757812,  0.00836182, -0.21386719])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 - 2s - loss: 0.6784 - acc: 0.5714 - 2s/epoch - 2s/step\n",
      "Epoch 2/100\n",
      "1/1 - 0s - loss: 0.6604 - acc: 0.7143 - 4ms/epoch - 4ms/step\n",
      "Epoch 3/100\n",
      "1/1 - 0s - loss: 0.6429 - acc: 0.7143 - 13ms/epoch - 13ms/step\n",
      "Epoch 4/100\n",
      "1/1 - 0s - loss: 0.6259 - acc: 0.8571 - 4ms/epoch - 4ms/step\n",
      "Epoch 5/100\n",
      "1/1 - 0s - loss: 0.6095 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
      "Epoch 6/100\n",
      "1/1 - 0s - loss: 0.5936 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
      "Epoch 7/100\n",
      "1/1 - 0s - loss: 0.5782 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
      "Epoch 8/100\n",
      "1/1 - 0s - loss: 0.5634 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
      "Epoch 9/100\n",
      "1/1 - 0s - loss: 0.5490 - acc: 1.0000 - 12ms/epoch - 12ms/step\n",
      "Epoch 10/100\n",
      "1/1 - 0s - loss: 0.5351 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 11/100\n",
      "1/1 - 0s - loss: 0.5217 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
      "Epoch 12/100\n",
      "1/1 - 0s - loss: 0.5087 - acc: 1.0000 - 13ms/epoch - 13ms/step\n",
      "Epoch 13/100\n",
      "1/1 - 0s - loss: 0.4962 - acc: 1.0000 - 14ms/epoch - 14ms/step\n",
      "Epoch 14/100\n",
      "1/1 - 0s - loss: 0.4840 - acc: 1.0000 - 12ms/epoch - 12ms/step\n",
      "Epoch 15/100\n",
      "1/1 - 0s - loss: 0.4723 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
      "Epoch 16/100\n",
      "1/1 - 0s - loss: 0.4610 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 17/100\n",
      "1/1 - 0s - loss: 0.4500 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
      "Epoch 18/100\n",
      "1/1 - 0s - loss: 0.4394 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
      "Epoch 19/100\n",
      "1/1 - 0s - loss: 0.4291 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
      "Epoch 20/100\n",
      "1/1 - 0s - loss: 0.4192 - acc: 1.0000 - 12ms/epoch - 12ms/step\n",
      "Epoch 21/100\n",
      "1/1 - 0s - loss: 0.4096 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
      "Epoch 22/100\n",
      "1/1 - 0s - loss: 0.4003 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 23/100\n",
      "1/1 - 0s - loss: 0.3912 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
      "Epoch 24/100\n",
      "1/1 - 0s - loss: 0.3825 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 25/100\n",
      "1/1 - 0s - loss: 0.3740 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
      "Epoch 26/100\n",
      "1/1 - 0s - loss: 0.3658 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 27/100\n",
      "1/1 - 0s - loss: 0.3578 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
      "Epoch 28/100\n",
      "1/1 - 0s - loss: 0.3501 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 29/100\n",
      "1/1 - 0s - loss: 0.3426 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
      "Epoch 30/100\n",
      "1/1 - 0s - loss: 0.3353 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 31/100\n",
      "1/1 - 0s - loss: 0.3282 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
      "Epoch 32/100\n",
      "1/1 - 0s - loss: 0.3214 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 33/100\n",
      "1/1 - 0s - loss: 0.3147 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
      "Epoch 34/100\n",
      "1/1 - 0s - loss: 0.3083 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 35/100\n",
      "1/1 - 0s - loss: 0.3020 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
      "Epoch 36/100\n",
      "1/1 - 0s - loss: 0.2959 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 37/100\n",
      "1/1 - 0s - loss: 0.2900 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
      "Epoch 38/100\n",
      "1/1 - 0s - loss: 0.2842 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 39/100\n",
      "1/1 - 0s - loss: 0.2787 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
      "Epoch 40/100\n",
      "1/1 - 0s - loss: 0.2732 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 41/100\n",
      "1/1 - 0s - loss: 0.2680 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
      "Epoch 42/100\n",
      "1/1 - 0s - loss: 0.2629 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 43/100\n",
      "1/1 - 0s - loss: 0.2579 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
      "Epoch 44/100\n",
      "1/1 - 0s - loss: 0.2530 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 45/100\n",
      "1/1 - 0s - loss: 0.2483 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
      "Epoch 46/100\n",
      "1/1 - 0s - loss: 0.2437 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 47/100\n",
      "1/1 - 0s - loss: 0.2393 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
      "Epoch 48/100\n",
      "1/1 - 0s - loss: 0.2350 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 49/100\n",
      "1/1 - 0s - loss: 0.2307 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
      "Epoch 50/100\n",
      "1/1 - 0s - loss: 0.2266 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
      "Epoch 51/100\n",
      "1/1 - 0s - loss: 0.2226 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
      "Epoch 52/100\n",
      "1/1 - 0s - loss: 0.2187 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
      "Epoch 53/100\n",
      "1/1 - 0s - loss: 0.2149 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 54/100\n",
      "1/1 - 0s - loss: 0.2112 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
      "Epoch 55/100\n",
      "1/1 - 0s - loss: 0.2076 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
      "Epoch 56/100\n",
      "1/1 - 0s - loss: 0.2041 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
      "Epoch 57/100\n",
      "1/1 - 0s - loss: 0.2007 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
      "Epoch 58/100\n",
      "1/1 - 0s - loss: 0.1974 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
      "Epoch 59/100\n",
      "1/1 - 0s - loss: 0.1941 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
      "Epoch 60/100\n",
      "1/1 - 0s - loss: 0.1910 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
      "Epoch 61/100\n",
      "1/1 - 0s - loss: 0.1879 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 62/100\n",
      "1/1 - 0s - loss: 0.1849 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
      "Epoch 63/100\n",
      "1/1 - 0s - loss: 0.1819 - acc: 1.0000 - 11ms/epoch - 11ms/step\n",
      "Epoch 64/100\n",
      "1/1 - 0s - loss: 0.1791 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
      "Epoch 65/100\n",
      "1/1 - 0s - loss: 0.1763 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 66/100\n",
      "1/1 - 0s - loss: 0.1735 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
      "Epoch 67/100\n",
      "1/1 - 0s - loss: 0.1709 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 68/100\n",
      "1/1 - 0s - loss: 0.1683 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
      "Epoch 69/100\n",
      "1/1 - 0s - loss: 0.1657 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
      "Epoch 70/100\n",
      "1/1 - 0s - loss: 0.1632 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
      "Epoch 71/100\n",
      "1/1 - 0s - loss: 0.1608 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
      "Epoch 72/100\n",
      "1/1 - 0s - loss: 0.1584 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
      "Epoch 73/100\n",
      "1/1 - 0s - loss: 0.1561 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
      "Epoch 74/100\n",
      "1/1 - 0s - loss: 0.1538 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
      "Epoch 75/100\n",
      "1/1 - 0s - loss: 0.1516 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
      "Epoch 76/100\n",
      "1/1 - 0s - loss: 0.1495 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
      "Epoch 77/100\n",
      "1/1 - 0s - loss: 0.1474 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
      "Epoch 78/100\n",
      "1/1 - 0s - loss: 0.1453 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
      "Epoch 79/100\n",
      "1/1 - 0s - loss: 0.1433 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
      "Epoch 80/100\n",
      "1/1 - 0s - loss: 0.1413 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
      "Epoch 81/100\n",
      "1/1 - 0s - loss: 0.1393 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
      "Epoch 82/100\n",
      "1/1 - 0s - loss: 0.1374 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
      "Epoch 83/100\n",
      "1/1 - 0s - loss: 0.1356 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 84/100\n",
      "1/1 - 0s - loss: 0.1338 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
      "Epoch 85/100\n",
      "1/1 - 0s - loss: 0.1320 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 86/100\n",
      "1/1 - 0s - loss: 0.1302 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
      "Epoch 87/100\n",
      "1/1 - 0s - loss: 0.1285 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 88/100\n",
      "1/1 - 0s - loss: 0.1268 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
      "Epoch 89/100\n",
      "1/1 - 0s - loss: 0.1252 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
      "Epoch 90/100\n",
      "1/1 - 0s - loss: 0.1236 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
      "Epoch 91/100\n",
      "1/1 - 0s - loss: 0.1220 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
      "Epoch 92/100\n",
      "1/1 - 0s - loss: 0.1205 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 93/100\n",
      "1/1 - 0s - loss: 0.1190 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
      "Epoch 94/100\n",
      "1/1 - 0s - loss: 0.1175 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 95/100\n",
      "1/1 - 0s - loss: 0.1160 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
      "Epoch 96/100\n",
      "1/1 - 0s - loss: 0.1146 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
      "Epoch 97/100\n",
      "1/1 - 0s - loss: 0.1132 - acc: 1.0000 - 3ms/epoch - 3ms/step\n",
      "Epoch 98/100\n",
      "1/1 - 0s - loss: 0.1118 - acc: 1.0000 - 4ms/epoch - 4ms/step\n",
      "Epoch 99/100\n",
      "1/1 - 0s - loss: 0.1105 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 100/100\n",
      "1/1 - 0s - loss: 0.1092 - acc: 1.0000 - 3ms/epoch - 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ae2301eac0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten, Input\n",
    "\n",
    "max_len = 4\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(max_len, ), dtype='int32'))\n",
    "e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_len, trainable=False)\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.fit(x_train, y_train, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0 (default, Nov  6 2019, 16:00:02) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "96512990021685504d4683198faad895f5cd0e4b7b1aa29365fef97d0a48eb34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
