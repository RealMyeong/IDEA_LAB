{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>ko</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I was hoping to buy some plastic furniture for...</td>\n",
       "      <td>플라스틱 가구를 도매로 사고 싶었습니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You eat yours. Let's not force each other.</td>\n",
       "      <td>너는 너 거 먹어, 강요하지 말자, 서로.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&gt;Be careful when you flip it over.</td>\n",
       "      <td>&gt;넘기는 거 조심해.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Well, we just really wanted a bigger dining an...</td>\n",
       "      <td>글쎄, 우리는 더 큰 식당과 거실 공간을 정말로 원했습니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How long has your company been doing business ...</td>\n",
       "      <td>당신의 회사는 얼마나 오랫동안 사업을 해왔나요?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>I badly need the money, because I will use it ...</td>\n",
       "      <td>돈이 절실히 필요합니다, 그 돈이 사업에 쓰일 것이기 때문이다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>Yes, I would love to arrange a meeting with you.</td>\n",
       "      <td>네, 당신과의 회의를 잡고 싶습니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>&gt; It's so touching.</td>\n",
       "      <td>&gt; 너무 따뜻해 이런 거.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>The price ranges from 729 dollars to 850 dollars.</td>\n",
       "      <td>가격은 729달러에서 850달러까지 다양합니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>Please explain this series of events.</td>\n",
       "      <td>이 일련의 사건들에 대해서 해명을 해주세요.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      en  \\\n",
       "0      I was hoping to buy some plastic furniture for...   \n",
       "1             You eat yours. Let's not force each other.   \n",
       "2                     >Be careful when you flip it over.   \n",
       "3      Well, we just really wanted a bigger dining an...   \n",
       "4      How long has your company been doing business ...   \n",
       "...                                                  ...   \n",
       "99995  I badly need the money, because I will use it ...   \n",
       "99996   Yes, I would love to arrange a meeting with you.   \n",
       "99997                                > It's so touching.   \n",
       "99998  The price ranges from 729 dollars to 850 dollars.   \n",
       "99999              Please explain this series of events.   \n",
       "\n",
       "                                        ko  \n",
       "0                   플라스틱 가구를 도매로 사고 싶었습니다.  \n",
       "1                  너는 너 거 먹어, 강요하지 말자, 서로.  \n",
       "2                              >넘기는 거 조심해.  \n",
       "3        글쎄, 우리는 더 큰 식당과 거실 공간을 정말로 원했습니다.  \n",
       "4               당신의 회사는 얼마나 오랫동안 사업을 해왔나요?  \n",
       "...                                    ...  \n",
       "99995  돈이 절실히 필요합니다, 그 돈이 사업에 쓰일 것이기 때문이다.  \n",
       "99996                 네, 당신과의 회의를 잡고 싶습니다.  \n",
       "99997                       > 너무 따뜻해 이런 거.  \n",
       "99998           가격은 729달러에서 850달러까지 다양합니다.  \n",
       "99999             이 일련의 사건들에 대해서 해명을 해주세요.  \n",
       "\n",
       "[100000 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "file_path = 'C:/Users/Myeong/dding/data/kor-eng_AIHUB.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "data.drop_duplicates(inplace=True)\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "data = data.sample(100000)\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>ko</th>\n",
       "      <th>en_pre</th>\n",
       "      <th>ko_pre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I was hoping to buy some plastic furniture for...</td>\n",
       "      <td>플라스틱 가구를 도매로 사고 싶었습니다.</td>\n",
       "      <td>i was hoping to buy some plastic furniture for...</td>\n",
       "      <td>&lt;start&gt; 플라스틱 가구 를 도매 로 사 고 싶 었 습니다 . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You eat yours. Let's not force each other.</td>\n",
       "      <td>너는 너 거 먹어, 강요하지 말자, 서로.</td>\n",
       "      <td>you eat yours . let s not force each other .</td>\n",
       "      <td>&lt;start&gt; 너 는 너 거 먹 어 , 강요 하 지 말 자 , 서로 . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&gt;Be careful when you flip it over.</td>\n",
       "      <td>&gt;넘기는 거 조심해.</td>\n",
       "      <td>be careful when you flip it over .</td>\n",
       "      <td>&lt;start&gt; 넘기 는 거 조심 해 . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Well, we just really wanted a bigger dining an...</td>\n",
       "      <td>글쎄, 우리는 더 큰 식당과 거실 공간을 정말로 원했습니다.</td>\n",
       "      <td>well , we just really wanted a bigger dining a...</td>\n",
       "      <td>&lt;start&gt; 글쎄 , 우리 는 더 큰 식당 과 거실 공간 을 정말로 원했 습니다 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How long has your company been doing business ...</td>\n",
       "      <td>당신의 회사는 얼마나 오랫동안 사업을 해왔나요?</td>\n",
       "      <td>how long has your company been doing business ...</td>\n",
       "      <td>&lt;start&gt; 당신 의 회사 는 얼마나 오랫동안 사업 을 해 왔 나요 ? &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>I badly need the money, because I will use it ...</td>\n",
       "      <td>돈이 절실히 필요합니다, 그 돈이 사업에 쓰일 것이기 때문이다.</td>\n",
       "      <td>i badly need the money , because i will use it...</td>\n",
       "      <td>&lt;start&gt; 돈 이 절실히 필요 합니다 , 그 돈 이 사업 에 쓰일 것 이 기 때...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>Yes, I would love to arrange a meeting with you.</td>\n",
       "      <td>네, 당신과의 회의를 잡고 싶습니다.</td>\n",
       "      <td>yes , i would love to arrange a meeting with y...</td>\n",
       "      <td>&lt;start&gt; 네 , 당신 과 의 회의 를 잡 고 싶 습니다 . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>&gt; It's so touching.</td>\n",
       "      <td>&gt; 너무 따뜻해 이런 거.</td>\n",
       "      <td>it s so touching .</td>\n",
       "      <td>&lt;start&gt; 너무 따뜻 해 이런 거 . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>The price ranges from 729 dollars to 850 dollars.</td>\n",
       "      <td>가격은 729달러에서 850달러까지 다양합니다.</td>\n",
       "      <td>the price ranges from 729 dollars to 850 dolla...</td>\n",
       "      <td>&lt;start&gt; 가격 은 729 달러 에서 850 달러 까지 다양 합니다 . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>Please explain this series of events.</td>\n",
       "      <td>이 일련의 사건들에 대해서 해명을 해주세요.</td>\n",
       "      <td>please explain this series of events .</td>\n",
       "      <td>&lt;start&gt; 이 일련 의 사건 들 에 대해서 해명 을 해 주 세요 . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      en  \\\n",
       "0      I was hoping to buy some plastic furniture for...   \n",
       "1             You eat yours. Let's not force each other.   \n",
       "2                     >Be careful when you flip it over.   \n",
       "3      Well, we just really wanted a bigger dining an...   \n",
       "4      How long has your company been doing business ...   \n",
       "...                                                  ...   \n",
       "99995  I badly need the money, because I will use it ...   \n",
       "99996   Yes, I would love to arrange a meeting with you.   \n",
       "99997                                > It's so touching.   \n",
       "99998  The price ranges from 729 dollars to 850 dollars.   \n",
       "99999              Please explain this series of events.   \n",
       "\n",
       "                                        ko  \\\n",
       "0                   플라스틱 가구를 도매로 사고 싶었습니다.   \n",
       "1                  너는 너 거 먹어, 강요하지 말자, 서로.   \n",
       "2                              >넘기는 거 조심해.   \n",
       "3        글쎄, 우리는 더 큰 식당과 거실 공간을 정말로 원했습니다.   \n",
       "4               당신의 회사는 얼마나 오랫동안 사업을 해왔나요?   \n",
       "...                                    ...   \n",
       "99995  돈이 절실히 필요합니다, 그 돈이 사업에 쓰일 것이기 때문이다.   \n",
       "99996                 네, 당신과의 회의를 잡고 싶습니다.   \n",
       "99997                       > 너무 따뜻해 이런 거.   \n",
       "99998           가격은 729달러에서 850달러까지 다양합니다.   \n",
       "99999             이 일련의 사건들에 대해서 해명을 해주세요.   \n",
       "\n",
       "                                                  en_pre  \\\n",
       "0      i was hoping to buy some plastic furniture for...   \n",
       "1           you eat yours . let s not force each other .   \n",
       "2                     be careful when you flip it over .   \n",
       "3      well , we just really wanted a bigger dining a...   \n",
       "4      how long has your company been doing business ...   \n",
       "...                                                  ...   \n",
       "99995  i badly need the money , because i will use it...   \n",
       "99996  yes , i would love to arrange a meeting with y...   \n",
       "99997                                 it s so touching .   \n",
       "99998  the price ranges from 729 dollars to 850 dolla...   \n",
       "99999             please explain this series of events .   \n",
       "\n",
       "                                                  ko_pre  \n",
       "0             <start> 플라스틱 가구 를 도매 로 사 고 싶 었 습니다 . <end>  \n",
       "1          <start> 너 는 너 거 먹 어 , 강요 하 지 말 자 , 서로 . <end>  \n",
       "2                            <start> 넘기 는 거 조심 해 . <end>  \n",
       "3      <start> 글쎄 , 우리 는 더 큰 식당 과 거실 공간 을 정말로 원했 습니다 ...  \n",
       "4         <start> 당신 의 회사 는 얼마나 오랫동안 사업 을 해 왔 나요 ? <end>  \n",
       "...                                                  ...  \n",
       "99995  <start> 돈 이 절실히 필요 합니다 , 그 돈 이 사업 에 쓰일 것 이 기 때...  \n",
       "99996          <start> 네 , 당신 과 의 회의 를 잡 고 싶 습니다 . <end>  \n",
       "99997                       <start> 너무 따뜻 해 이런 거 . <end>  \n",
       "99998    <start> 가격 은 729 달러 에서 850 달러 까지 다양 합니다 . <end>  \n",
       "99999      <start> 이 일련 의 사건 들 에 대해서 해명 을 해 주 세요 . <end>  \n",
       "\n",
       "[100000 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 소문자화\n",
    "# 2. 구두점과 단어 사이에 공백 추가\n",
    "# 3. 공백 2개 이상 -> 공백 1개\n",
    "# 4. <start>, <end> 토큰 추가\n",
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab(dicpath = 'C:/Users/Myeong/anaconda3/envs/jin/etc/mecab-ko-dic')\n",
    "\n",
    "def preprocessing(s, ko=False):\n",
    "    # s = unidecode(s)\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s)\n",
    "    s = re.sub(r'[\" \"]+', \" \", s)\n",
    "    s = re.sub(r\"[^ㄱ-ㅎ가-힣ㅏ-ㅣa-zA-Z0-9?.,!¿]+\", \" \", s)\n",
    "\n",
    "    # 한국어는 따로 형태소 분석을 진행해줌\n",
    "    # 한국어를 target 으로 할 예정이기 때문에 \n",
    "    if ko:\n",
    "        s = mecab.morphs(s)\n",
    "        s = ' '.join(s)\n",
    "        s = '<start> ' + s + ' <end>'\n",
    "    \n",
    "    s = s.strip()\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "# 각 컬럼 전처리\n",
    "data['en_pre'] = data['en'].apply(preprocessing)\n",
    "data['ko_pre'] = data['ko'].apply(lambda x : preprocessing(x, ko=True))\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106 76\n"
     ]
    }
   ],
   "source": [
    "ko_max = 0\n",
    "en_max = 0\n",
    "for i in range(len(data)):\n",
    "    ko_max = max(ko_max, len(data['ko_pre'][i].split()))\n",
    "    en_max = max(en_max, len(data['en_pre'][i].split()))\n",
    "\n",
    "print(ko_max, en_max)\n",
    "max_len = max(ko_max, en_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   5   53  899 ...    0    0    0]\n",
      " [   6  189 1901 ...    0    0    0]\n",
      " [  23 1094   66 ...    0    0    0]\n",
      " ...\n",
      " [   9   19   28 ...    0    0    0]\n",
      " [   3  132 6544 ...    0    0    0]\n",
      " [  50  817   17 ...    0    0    0]] <keras.preprocessing.text.Tokenizer object at 0x000002873833A280>\n",
      "[[   1  816  910 ...    0    0    0]\n",
      " [   1  331    5 ...    0    0    0]\n",
      " [   1 5792    5 ...    0    0    0]\n",
      " ...\n",
      " [   1  166 1046 ...    0    0    0]\n",
      " [   1  168   13 ...    0    0    0]\n",
      " [   1    6 4191 ...    0    0    0]] <keras.preprocessing.text.Tokenizer object at 0x00000287939E45B0>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus, maxlen, padding='post', target=False,):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=' ')\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding=padding, maxlen=maxlen)\n",
    "\n",
    "    print(tensor, tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "en_tensor, en_tokenizer = tokenize(data['en_pre'], maxlen=max_len)\n",
    "ko_tensor, ko_tokenizer = tokenize(data['ko_pre'], maxlen=max_len)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100000, 106), (100000, 106))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_tensor.shape, en_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   5,   53,  899, ...,    0,    0,    0],\n",
       "       [   6,  189, 1901, ...,    0,    0,    0],\n",
       "       [  23, 1094,   66, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   9,   19,   28, ...,    0,    0,    0],\n",
       "       [   3,  132, 6544, ...,    0,    0,    0],\n",
       "       [  50,  817,   17, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "23212 26504\n"
     ]
    }
   ],
   "source": [
    "print(ko_tokenizer.word_index['<start>'])\n",
    "print(ko_tokenizer.word_index['<end>'])\n",
    "\n",
    "en_vocab_size = len(en_tokenizer.word_index) \n",
    "ko_vocab_size = len(ko_tokenizer.word_index) \n",
    "\n",
    "print(en_vocab_size, ko_vocab_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, (2*int(i))/d_model)\n",
    "    \n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "    \n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "    # 짝수 인덱스 sin함수\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    # 홀수 인덱스 cos함수\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "    return sinusoid_table\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # 헤드의 개수\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # 헤드당 차원의 크기\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        # Q, K, V 가중치\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    def scaled_dot_product_attention(self, q, k, v, mask):\n",
    "        # query shape : (batch_size, num_heads, length, d_model/num_heads)\n",
    "        # key shape : (batch_size, num_heads, length, depth)\n",
    "        # value shape : (batch_size, num_heads, length, depth)\n",
    "        # padding mask shape : (batch_size, 1, 1, key length)\n",
    "        d_k = tf.cast(k.shape[-1], tf.float32)\n",
    "\n",
    "        # q, k dot product\n",
    "        qk = tf.matmul(q, k, transpose_b=True)\n",
    "        scaled_qk = qk / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_qk += (mask * -1e9)\n",
    "\n",
    "        # attention weights shape : (batch_size, num_heads, query length, key length)\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "\n",
    "        # output shape : (batch_size, num_heads, query length, d_model/num_heads)\n",
    "        out = tf.matmul(attentions, v)\n",
    "\n",
    "        return out, attentions\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        # x shape : (batch_size, length, d_model)\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # split_x shape : (batch_size, length, num_heads, d_model/num_heads)\n",
    "        split_x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "\n",
    "        # 각 배치의 각 헤드당 (length X depth)가 있도록 변경\n",
    "        # split_x shape : (batch_size, num_heads, length, depth)\n",
    "        split_x = tf.transpose(split_x, perm = [0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        # x shape : (batch_size, num_heads, length, depth)\n",
    "        batch_size = x.shape[0]\n",
    "        # x shape to (batch_size, length, num_heads, depth)\n",
    "        x = tf.transpose(x, perm = [0, 2, 1, 3])\n",
    "\n",
    "        # combined shape : (batch_size, length, d_model)\n",
    "        combined_x = tf.reshape(x, (batch_size, -1, self.d_model))\n",
    "        return combined_x\n",
    "    \n",
    "    def call(self, q, k, v, mask):\n",
    "        wq = self.wq(q)\n",
    "        wk = self.wk(k)\n",
    "        wv = self.wv(v)\n",
    "\n",
    "        # split heads\n",
    "        wq_split = self.split_heads(wq)\n",
    "        wk_split = self.split_heads(wk)\n",
    "        wv_split = self.split_heads(wv)\n",
    "\n",
    "        # scaled dot product attention\n",
    "        out, attention_weights = self.scaled_dot_product_attention(wq_split, wk_split, wv_split, mask)\n",
    "\n",
    "        # combine heads\n",
    "        out = self.combine_heads(out)\n",
    "\n",
    "        # linear out\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.w1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.w2 = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    def call(self, x):\n",
    "        out = self.w1(x)\n",
    "        out = self.w2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, mask):\n",
    "\n",
    "        # Multihead Attention\n",
    "        residual = x\n",
    "        out = self.norm1(x)\n",
    "        out, attention = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        # Position-Wise Feed Forward Network\n",
    "        residual = out\n",
    "        out = self.norm2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        # Masked Multihead Attention\n",
    "        residual = x\n",
    "        out = self.norm1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        # Encoder-Decoder Multihead Attention\n",
    "        residual = out\n",
    "        out = self.norm2(out)\n",
    "        # Encoder output과 decoder output을 입력으로 넣어주는 부분\n",
    "        # 마스킹을 통하여 leftward information flow를 유지해줌\n",
    "        out, enc_dec_attn = self.enc_dec_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        # Position-Wise Feed Forward Network\n",
    "        residual = out\n",
    "        out = self.norm3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, enc_dec_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(n_layers)]\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "\n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(n_layers)]\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "\n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 d_ff,\n",
    "                 src_vocab_size,\n",
    "                 tgt_vocab_size,\n",
    "                 pos_len,\n",
    "                 dropout=0.1,\n",
    "                 shared=True):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, num_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, num_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "        \n",
    "        self.shared = shared\n",
    "        if shared: self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "        \n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "    \n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "\n",
    "        dec_out, dec_attns, dec_enc_attns = self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "\n",
    "        logits = self.fc(dec_out)\n",
    "\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26504"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# batch, length = 16, 20\n",
    "# src_padding = 5\n",
    "# tgt_padding = 15\n",
    "\n",
    "# src_pad = tf.zeros(shape=(batch, src_padding))\n",
    "# tgt_pad = tf.zeros(shape=(batch, tgt_padding))\n",
    "\n",
    "# sample_data2 = tf.ones(shape=(batch, length))\n",
    "\n",
    "# sample_src = tf.concat([sample_data2, src_pad], axis=-1)\n",
    "# sample_tgt = tf.concat([sample_data2, tgt_pad], axis=-1)\n",
    "\n",
    "# enc_mask, dec_enc_mask, dec_mask = generate_mask(sample_src, sample_tgt)\n",
    "\n",
    "# fig = plt.figure(figsize=(7,7))\n",
    "\n",
    "# ax1 = fig.add_subplot(131)\n",
    "# ax2 = fig.add_subplot(132)\n",
    "# ax3 = fig.add_subplot(133)\n",
    "\n",
    "# ax1.set_title('1) Encoder Mask')\n",
    "# ax2.set_title('2) Encoder-Decoder Mask')\n",
    "# ax3.set_title('3) Decoder Mask')\n",
    "\n",
    "# ax1.imshow(enc_mask[:3, 0, 0].numpy(), cmap='Dark2')\n",
    "# ax2.imshow(dec_enc_mask[0, 0].numpy(), cmap='Dark2')\n",
    "# ax3.imshow(dec_mask[0, 0].numpy(), cmap='Dark2')\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = LearningRateScheduler(512)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                     beta_1 = 0.9,\n",
    "                                     beta_2 = 0.98,\n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "transformer = Transformer(n_layers=4,\n",
    "                          d_model=512,\n",
    "                          num_heads=8,\n",
    "                          d_ff = 2048,\n",
    "                          src_vocab_size = len(en_tokenizer.word_index),\n",
    "                          tgt_vocab_size = len(ko_tokenizer.word_index),\n",
    "                          pos_len = 200,\n",
    "                          dropout = 0.1,\n",
    "                          shared=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    gold = tgt[:, 1:]\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions[:, :-1])\n",
    "        \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention 시각화 함수\n",
    "\n",
    "def visualize_attention(src, tgt, enc_attns, dec_attns, dec_enc_attns):\n",
    "    def draw(data, ax, x=\"auto\", y=\"auto\"):\n",
    "        import seaborn\n",
    "        seaborn.heatmap(data, \n",
    "                        square=True,\n",
    "                        vmin=0.0, \n",
    "                        vmax=1.0, \n",
    "                        cbar=False, \n",
    "                        ax=ax,\n",
    "                        xticklabels=x,\n",
    "                        yticklabels=y)\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Encoder Layer\", layer + 1)\n",
    "        for h in range(4):\n",
    "            draw(enc_attns[layer][0, h, :len(src), :len(src)], axs[h], src, src)\n",
    "        plt.show()\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Decoder Self Layer\", layer+1)\n",
    "        for h in range(4):\n",
    "            draw(dec_attns[layer][0, h, :len(tgt), :len(tgt)], axs[h], tgt, tgt)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Decoder Src Layer\", layer+1)\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        for h in range(4):\n",
    "            draw(dec_enc_attns[layer][0, h, :len(tgt), :len(src)], axs[h], src, tgt)\n",
    "        plt.show()\n",
    "\n",
    "# 번역 생성 함수\n",
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocessing(sentence)\n",
    "\n",
    "    pieces = sentence.split()\n",
    "    tokens = en_tokenizer.texts_to_sequences(sentence)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=en_tensor.shape[-1],\n",
    "                                                           padding='post')\n",
    "    ids = []\n",
    "    output = tf.expand_dims([ko_tokenizer.word_index['<start>']], 0)\n",
    "    for i in range(ko_tensor.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(_input,\n",
    "                                                                 output,\n",
    "                                                                 enc_padding_mask,\n",
    "                                                                 combined_mask,\n",
    "                                                                 dec_padding_mask)\n",
    "        predicted_id = tf.argmax(tf.math.softmax(predictions, axis=-1)[0,-1]).numpy().item()\n",
    "\n",
    "        if ko_tokenizer.word_index['<end>'] == predicted_id:\n",
    "            result = ko_tokenizer.sequences_to_texts(ids)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "        \n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "    \n",
    "    result = ko_tokenizer.sequences_to_texts(ids)\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention=False):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "\n",
    "    print(\"Input : %s\"%(sentence))\n",
    "    print(\"Predicted translation : {}\".format(result))\n",
    "\n",
    "    if plot_attention:\n",
    "        visualize_attention(pieces, result.split(), enc_attns, dec_attns, dec_enc_attns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1: 100%|██████████| 1563/1563 [04:40<00:00,  5.56it/s, Loss nan]   \n",
      "c:\\Users\\Myeong\\anaconda3\\envs\\jin\\lib\\site-packages\\keras\\utils\\data_utils.py:1012: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sample_shape = np.asarray(x).shape[1:]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (62,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m     t\u001b[39m.\u001b[39mset_postfix_str(\u001b[39m\"\u001b[39m\u001b[39mLoss \u001b[39m\u001b[39m%.4f\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m(total_loss\u001b[39m.\u001b[39mnumpy() \u001b[39m/\u001b[39m (batch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)))\n\u001b[0;32m     28\u001b[0m \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m examples:\n\u001b[1;32m---> 29\u001b[0m     translate(example, transformer, en_tokenizer, ko_tokenizer)\n",
      "Cell \u001b[1;32mIn[21], line 3\u001b[0m, in \u001b[0;36mtranslate\u001b[1;34m(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtranslate\u001b[39m(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m      2\u001b[0m     pieces, result, enc_attns, dec_attns, dec_enc_attns \u001b[39m=\u001b[39m \\\n\u001b[1;32m----> 3\u001b[0m     evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n\u001b[0;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mInput : \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m(sentence))\n\u001b[0;32m      6\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPredicted translation : \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(result))\n",
      "Cell \u001b[1;32mIn[20], line 42\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(sentence, model, src_tokenizer, tgt_tokenizer)\u001b[0m\n\u001b[0;32m     39\u001b[0m pieces \u001b[39m=\u001b[39m sentence\u001b[39m.\u001b[39msplit()\n\u001b[0;32m     40\u001b[0m tokens \u001b[39m=\u001b[39m en_tokenizer\u001b[39m.\u001b[39mtexts_to_sequences(sentence)\n\u001b[1;32m---> 42\u001b[0m _input \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mpreprocessing\u001b[39m.\u001b[39;49msequence\u001b[39m.\u001b[39;49mpad_sequences([tokens],\n\u001b[0;32m     43\u001b[0m                                                        maxlen\u001b[39m=\u001b[39;49men_tensor\u001b[39m.\u001b[39;49mshape[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m],\n\u001b[0;32m     44\u001b[0m                                                        padding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     45\u001b[0m ids \u001b[39m=\u001b[39m []\n\u001b[0;32m     46\u001b[0m output \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mexpand_dims([ko_tokenizer\u001b[39m.\u001b[39mword_index[\u001b[39m'\u001b[39m\u001b[39m<start>\u001b[39m\u001b[39m'\u001b[39m]], \u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Myeong\\anaconda3\\envs\\jin\\lib\\site-packages\\keras\\utils\\data_utils.py:1041\u001b[0m, in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m   1038\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTruncating type \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mtruncating\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m not understood\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   1040\u001b[0m \u001b[39m# check `trunc` has expected shape\u001b[39;00m\n\u001b[1;32m-> 1041\u001b[0m trunc \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(trunc, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m   1042\u001b[0m \u001b[39mif\u001b[39;00m trunc\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m:] \u001b[39m!=\u001b[39m sample_shape:\n\u001b[0;32m   1043\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mShape of sample \u001b[39m\u001b[39m{\u001b[39;00mtrunc\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m:]\u001b[39m}\u001b[39;00m\u001b[39m of sequence at \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1044\u001b[0m                    \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mposition \u001b[39m\u001b[39m{\u001b[39;00midx\u001b[39m}\u001b[39;00m\u001b[39m is different from expected shape \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1045\u001b[0m                    \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00msample_shape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (62,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "\n",
    "examples = list(data['en_pre'].sample(5).values)\n",
    "\n",
    "loss_ = []\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    idx_list = list(range(0, en_tensor.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = train_step(en_tensor[idx : idx + BATCH_SIZE],\n",
    "                                                                     ko_tensor[idx : idx + BATCH_SIZE],\n",
    "                                                                     transformer,\n",
    "                                                                     optimizer)\n",
    "        loss_.append(batch_loss)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        t.set_description_str(\"Epoch %2d\" %(epoch + 1))\n",
    "        t.set_postfix_str(\"Loss %.4f\" %(total_loss.numpy() / (batch+1)))\n",
    "    \n",
    "    for example in examples:\n",
    "        translate(example, transformer, en_tokenizer, ko_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m examples:\n\u001b[1;32m----> 2\u001b[0m         translate(example, transformer, en_tokenizer, ko_tokenizer)\n",
      "Cell \u001b[1;32mIn[24], line 3\u001b[0m, in \u001b[0;36mtranslate\u001b[1;34m(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtranslate\u001b[39m(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m      2\u001b[0m     pieces, result, enc_attns, dec_attns, dec_enc_attns \u001b[39m=\u001b[39m \\\n\u001b[1;32m----> 3\u001b[0m     evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n\u001b[0;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mInput : \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m(sentence))\n\u001b[0;32m      6\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPredicted translation : \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(result))\n",
      "Cell \u001b[1;32mIn[23], line 64\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(sentence, model, src_tokenizer, tgt_tokenizer)\u001b[0m\n\u001b[0;32m     61\u001b[0m     ids\u001b[39m.\u001b[39mappend(predicted_id)\n\u001b[0;32m     62\u001b[0m     output \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconcat([output, tf\u001b[39m.\u001b[39mexpand_dims([predicted_id], \u001b[39m0\u001b[39m)], axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 64\u001b[0m result \u001b[39m=\u001b[39m ko_tokenizer\u001b[39m.\u001b[39;49msequences_to_texts(ids)\n\u001b[0;32m     65\u001b[0m \u001b[39mreturn\u001b[39;00m pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
      "File \u001b[1;32mc:\\Users\\Myeong\\anaconda3\\envs\\jin\\lib\\site-packages\\keras\\preprocessing\\text.py:395\u001b[0m, in \u001b[0;36mTokenizer.sequences_to_texts\u001b[1;34m(self, sequences)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msequences_to_texts\u001b[39m(\u001b[39mself\u001b[39m, sequences):\n\u001b[0;32m    384\u001b[0m   \u001b[39m\"\"\"Transforms each sequence into a list of text.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[39m  Only top `num_words-1` most frequent words will be taken into account.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m \u001b[39m      A list of texts (strings)\u001b[39;00m\n\u001b[0;32m    394\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 395\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msequences_to_texts_generator(sequences))\n",
      "File \u001b[1;32mc:\\Users\\Myeong\\anaconda3\\envs\\jin\\lib\\site-packages\\keras\\preprocessing\\text.py:416\u001b[0m, in \u001b[0;36mTokenizer.sequences_to_texts_generator\u001b[1;34m(self, sequences)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m sequences:\n\u001b[0;32m    415\u001b[0m   vect \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 416\u001b[0m   \u001b[39mfor\u001b[39;00m num \u001b[39min\u001b[39;00m seq:\n\u001b[0;32m    417\u001b[0m     word \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex_word\u001b[39m.\u001b[39mget(num)\n\u001b[0;32m    418\u001b[0m     \u001b[39mif\u001b[39;00m word \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "for example in examples:\n",
    "        translate(example, transformer, en_tokenizer, ko_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  11837952  \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     multiple                  21606400  \n",
      "                                                                 \n",
      " dropout (Dropout)           multiple                  0         \n",
      "                                                                 \n",
      " encoder (Encoder)           multiple                  788992    \n",
      "                                                                 \n",
      " decoder (Decoder)           multiple                  0 (unused)\n",
      "                                                                 \n",
      " dense_64 (Dense)            multiple                  0 (unused)\n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34,233,344\n",
      "Trainable params: 34,233,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> it is an immersive product that serves a different purpose than a traditional product .  <end>',\n",
       " '<start> for accurate cost calculation , it would be better for your employees and our employees to check the facility usage status at the site .  <end>',\n",
       " '<start> thank you for your interest in our company s products , and please feel free to contact us if you have any questions before this contract .  <end>',\n",
       " '<start> no , that is not the case .  <end>',\n",
       " '<start> we are not sure whether we should accompany you with the data you have sent us .  <end>']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = list(data['en_pre'].sample(5))\n",
    "examples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0 (default, Nov  6 2019, 16:00:02) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "96512990021685504d4683198faad895f5cd0e4b7b1aa29365fef97d0a48eb34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
