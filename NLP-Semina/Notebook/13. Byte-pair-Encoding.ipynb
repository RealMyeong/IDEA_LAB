{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding, BPE\n",
    "---\n",
    "\n",
    "기계에게 아무리 많은 단어를 학습시켜도 세상의 모든 단어를 알려줄 수는 없습니다. 만약 기계가 모르는 단어가 등장하면 그 단어를 단어 집합에 없는 단어란 의미에서 해당 ```토큰을 UNK(Unknown Token)```라고 표현합니다.  \n",
    "\n",
    "이와 같이 모르는 단어로 인해 발생하는 문제를 ```OOV(Out-Of-Vocabulary) 문제```라고 합니다.\n",
    "\n",
    "Subword Segmentation 작업은 하나의 단어는 더 작은 단위의 의미있는 여러 서브워드들의 조합으로 구성된 경우가 많기 때문에, 하나의 단어를 여러 Subword로 분리해서 단어를 인코딩 및 임베딩 하겠다는 의도를 가진 전처리 작업입니다.  이를 통해 OOV나 희귀 단어, 신조어와 같은 문제를 완화시킬 수 있습니다.\n",
    "\n",
    "실제로 언어의 특성으로인해 영어나 한국어는 Subword Segmentation을 했을 때 어느정도 의미있는 단위로 나누는 것이 가능합니다. 이러한 작업을 하는 토크나이저를 Subword Tokenizer라고 합니다.\n",
    "\n",
    "## 1. BPE(Byte Pair Encoding)\n",
    "---\n",
    "아래와 같은 문자열이 주어졌을 때 BPE을 수행한다고 해봅시다.  \n",
    "```python\n",
    "aaabdaaabac\n",
    "```\n",
    "BPE은 기본적으로 연속적으로 가장 많이 등장한 byte pair를 찾아서 하나의 글자로 병합하는 방식을 수행합니다.  \n",
    "위 문장에서 가장 자주 등장하는 byte pair는 'aa'입니다. 이 'aa'라는 byte pair를 'Z'로 치환해보겠습니다.\n",
    "\n",
    "```python\n",
    "ZabdZabac\n",
    "Z = aa\n",
    "```\n",
    "\n",
    "한 번 치환한 문자열에서 가장 많이 등장하는 byte pair는 'ab'입니다. 이 'ab'를 'Y'로 치환해봅시다.\n",
    "\n",
    "```python\n",
    "ZYdZYac\n",
    "Y = ab\n",
    "Z = aa\n",
    "```\n",
    "\n",
    "이번에 가장 많이 등장하는 byte pair는 'ZY'입니다. 이를 'X'로 치환해봅시다.\n",
    "```python\n",
    "XdXac\n",
    "X = ZY\n",
    "Y = ab\n",
    "Z = aa\n",
    "```\n",
    "\n",
    "더 이상 병합할 byte pair가 없으므로 BPE는 위의 결과를 최종 결과로 하여 종료됩니다.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 자연어 처리에서의 BPE\n",
    "\n",
    "논문 : https://arxiv.org/pdf/1508.07909.pdf\n",
    "\n",
    "자연어 처리에서 BPE는 subword segmentation 알고리즘입니다. 요약하자면, character 단위에서 점차적으로 단어 집합(vocabulary)를 만들어 내는 bottom up 방식의 접근을 사용합니다.\n",
    "\n",
    "먼저 훈련 데이터에 있는 단어들을 모든 character또는 unicode 단위로 단어 집합(vocabulary)를 만들고, 가장 많이 등장하는 유니그램을 하나의 유니그램으로 통합합니다.\n",
    "\n",
    "1. BPE 알고리즘 사용\n",
    "\n",
    "기존 딕셔너리에 아래와 같이 저장되어 있다고 하겠습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# dictionary\n",
    "dict = {'low' : 5, 'lower' : 2, 'newest' : 6, 'widest' : 3}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 딕셔너리의 모든 단어들을 글자(character)단위로 분리합니다.\n",
    "\n",
    "```python\n",
    "l o w : 5, l o w e r : 2, n e w e s t : 6, w i d e s t : 3\n",
    "```\n",
    "딕셔너리를 참고로 한 초기 단어 집합(vocabulary)은 아래와 같습니다. 간단히 말해 초기 구성은 글자 단위로 분리된 상태입니다.\n",
    "\n",
    "```python\n",
    "l, o, w, e, r, n, s, t, i, d\n",
    "```\n",
    "BPE의 특징은 알고리즘의 동작을 몇 회 반복(iteration)할 것인지를 사용자가 정한다는 것입니다.\n",
    "\n",
    "여기서는 총 10회를 수행한다고 가정하겠습니다. 이는 가장 빈도수가 높은 유니그램의 쌍을 하나의 유니그램으로 통합하는 과정을 10회 반복한다는 말입니다.\n",
    "\n",
    "- 1회 : 딕셔너리를 참고 하였을 때 빈도수가 9로 가장 높은 (e, s)의 쌍을 es로 통합합니다.\n",
    "```python\n",
    "# dictionary update!\n",
    "l o w : 5,\n",
    "l o w e r : 2,\n",
    "n e w es t : 6,\n",
    "w i d es t : 3\n",
    "```\n",
    "\n",
    "```python\n",
    "# vocabulary update!\n",
    "l, o, w, e, r, n, s, t, i, d, es\n",
    "```\n",
    "\n",
    "- 2회 : 빈도수가 9로 가장 높은 (es, t)의 쌍을 est로 통합합니다.\n",
    "```python\n",
    "# dictionary update!\n",
    "l o w : 5,\n",
    "l o w e r : 2,\n",
    "n e w est : 6,\n",
    "w i d est : 3\n",
    "```\n",
    "\n",
    "```python\n",
    "# vocabulary update!\n",
    "l, o, w, e, r, n, s, t, i, d, es, est\n",
    "```\n",
    "\n",
    "- 3회 : 빈도수가 7로 가장 높은 (l, o)의 쌍을 lo로 통합합니다.\n",
    "```python\n",
    "# dictionary update!\n",
    "lo w : 5,\n",
    "lo w e r : 2,\n",
    "n e w est : 6,\n",
    "w i d est : 3\n",
    "```\n",
    "\n",
    "```python\n",
    "# vocabulary update!\n",
    "l, o, w, e, r, n, s, t, i, d, es, est, lo\n",
    "```\n",
    "\n",
    "지금까지와 같은 방식으로 총 10회 반복하였을 때의 딕셔너리와 단어 집합은 아래와 같습니다.\n",
    "\n",
    "```python\n",
    "# dictionary\n",
    "low : 5,\n",
    "low, e, r : 2,\n",
    "newest : 6,\n",
    "widest : 3\n",
    "```\n",
    "\n",
    "```python\n",
    "# vocabulary\n",
    "l, o, w, e, r, n, s, t, i, d, es, est, lo, low, ne, new, newest, wi, wid, widest\n",
    "```\n",
    "\n",
    "이 경우 테스트 과정에서 'lowest'라는 단어가 등장한다면, 기존에는 OOV에 해당하는 단어가 되었겠지만 BPE 알고리즘을 사용한 위의 단어 집합에서는 더 이상 'lowest'는 OOV가 아닙니다. 기계는 우선 'lowest'를 전부 글자 단위로 분할하고 'low'와 'est'를 찾아냅니다. 기계는 'lowest'를 'low'와 'est'두 단어로 인코딩 합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 코드 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Iteration  1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 pair들의 빈도수 :  {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 8, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('e', 's'): 9, ('s', 't'): 9, ('t', '</w>'): 9, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'e'): 3}\n",
      "new merge : ('e', 's')\n",
      "dictionary : {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 6, 'w i d es t </w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration  2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 pair들의 빈도수 :  {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'es'): 6, ('es', 't'): 9, ('t', '</w>'): 9, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'es'): 3}\n",
      "new merge : ('es', 't')\n",
      "dictionary : {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est </w>': 6, 'w i d est </w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration  3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 pair들의 빈도수 :  {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est'): 6, ('est', '</w>'): 9, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est'): 3}\n",
      "new merge : ('est', '</w>')\n",
      "dictionary : {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration  4"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 pair들의 빈도수 :  {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
      "new merge : ('l', 'o')\n",
      "dictionary : {'lo w </w>': 5, 'lo w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration  5"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 pair들의 빈도수 :  {('lo', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
      "new merge : ('lo', 'w')\n",
      "dictionary : {'low </w>': 5, 'low e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration  6"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 pair들의 빈도수 :  {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
      "new merge : ('n', 'e')\n",
      "dictionary : {'low </w>': 5, 'low e r </w>': 2, 'ne w est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration  7"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 pair들의 빈도수 :  {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('ne', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
      "new merge : ('ne', 'w')\n",
      "dictionary : {'low </w>': 5, 'low e r </w>': 2, 'new est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration  8"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 pair들의 빈도수 :  {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('new', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
      "new merge : ('new', 'est</w>')\n",
      "dictionary : {'low </w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration  9"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 pair들의 빈도수 :  {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
      "new merge : ('low', '</w>')\n",
      "dictionary : {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration  10"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 pair들의 빈도수 :  {('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
      "new merge : ('w', 'i')\n",
      "dictionary : {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'wi d est</w>': 3}\n"
     ]
    }
   ],
   "source": [
    "import re, collections\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "# BPE 수행 횟수\n",
    "num_merges = 10\n",
    "\n",
    "# dictionary\n",
    "# </w>는 단어 맨 끝에 붙이는 특수 문자\n",
    "dictionary = {'l o w </w>' : 5,\n",
    "              'l o w e r </w>' : 2,\n",
    "              'n e w e s t </w>' : 6,\n",
    "              'w i d e s t </w>' : 3}\n",
    "\n",
    "# BPE algorithm\n",
    "def get_stats(dictionary):\n",
    "    # 유니그램 pair들의 빈도수 카운트\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in dictionary.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i], symbols[i+1]] += freq\n",
    "    print('현재 pair들의 빈도수 : ', dict(pairs))\n",
    "    return pairs\n",
    "\n",
    "def merge_dictionary(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "bpe_codes = {}\n",
    "bpe_codes_reverse = {}\n",
    "\n",
    "for i in range(num_merges):\n",
    "    display(Markdown(\"### Iteration  {}\".format(i+1)))\n",
    "    pairs = get_stats(dictionary)\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    dictionary = merge_dictionary(best, dictionary)\n",
    "\n",
    "    bpe_codes[best] = i\n",
    "    bpe_codes_reverse[best[0] + best[1]] = best\n",
    "\n",
    "    print(\"new merge : {}\".format(best))\n",
    "    print(\"dictionary : {}\".format(dictionary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('e', 's'): 0, ('es', 't'): 1, ('est', '</w>'): 2, ('l', 'o'): 3, ('lo', 'w'): 4, ('n', 'e'): 5, ('ne', 'w'): 6, ('new', 'est</w>'): 7, ('low', '</w>'): 8, ('w', 'i'): 9}\n"
     ]
    }
   ],
   "source": [
    "# merge 기록 확인하기\n",
    "print(bpe_codes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOV 대처하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs(word):\n",
    "    \"\"\"\n",
    "    Return set of symbol pairs in a word\n",
    "    Word is represented as a tuple of symbols\n",
    "    (symbols being variable-length strings)\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "def encode(orig):\n",
    "    \"\"\"Encode word based on list of BPE merge operations, which are applied consecutively\"\"\"\n",
    "\n",
    "    word = tuple(orig) + ('</w>',)\n",
    "    display(Markdown(\"__word split into characters:__ <tt>{}</tt>\".format(word)))\n",
    "\n",
    "    pairs = get_pairs(word)\n",
    "\n",
    "    if not pairs:\n",
    "        return orig\n",
    "    \n",
    "    iteration = 0\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        display(Markdown(\"__Iteration {}:__\".format(iteration)))\n",
    "\n",
    "        print(\"bigrams in the word: {}\".format(pairs))\n",
    "        bigram = min(pairs, key = lambda pair : bpe_codes.get(pair, float('inf')))\n",
    "        print(\"condidate for merging : {}\".format(bigram))\n",
    "\n",
    "        if bigram not in bpe_codes:\n",
    "            display(Markdown(\"__Candidate not in BPE merges, algorithm stops.__\"))\n",
    "            break\n",
    "\n",
    "        first, second = bigram\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            try:\n",
    "                j = word.index(first, i)\n",
    "                new_word.extend(word[i:j])\n",
    "                i = j\n",
    "            except:\n",
    "                new_word.extend(word[i:])\n",
    "                break\n",
    "\n",
    "            if word[i] == first and i < len(word) -1 and word[i+1] == second:\n",
    "                new_word.append(first + second)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "        \n",
    "        new_word = tuple(new_word)\n",
    "        word = new_word\n",
    "        print(\"word after merging: {}\".format(word))\n",
    "        if len(word) == 1:\n",
    "            break\n",
    "        else:\n",
    "            pairs = get_pairs(word)\n",
    "    \n",
    "    # </w> 는 출력 안함\n",
    "    if word[-1] == '</w>':\n",
    "        word = word[:-1]\n",
    "    elif word[-1].endswith('</w>'):\n",
    "        word = word[:-1] + (word[-1].replace('</w>', ''),)\n",
    "    \n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "__word split into characters:__ <tt>('l', 'o', 'k', 'i', '</w>')</tt>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Iteration 1:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('k', 'i'), ('o', 'k'), ('l', 'o'), ('i', '</w>')}\n",
      "condidate for merging : ('l', 'o')\n",
      "word after merging: ('lo', 'k', 'i', '</w>')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "__Iteration 2:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('k', 'i'), ('lo', 'k'), ('i', '</w>')}\n",
      "condidate for merging : ('k', 'i')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "__Candidate not in BPE merges, algorithm stops.__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('lo', 'k', 'i')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode('loki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "__word split into characters:__ <tt>('l', 'o', 'w', 'e', 's', 't', '</w>')</tt>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Iteration 1:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('l', 'o'), ('t', '</w>'), ('o', 'w'), ('e', 's'), ('s', 't'), ('w', 'e')}\n",
      "condidate for merging : ('e', 's')\n",
      "word after merging: ('l', 'o', 'w', 'es', 't', '</w>')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "__Iteration 2:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('l', 'o'), ('t', '</w>'), ('o', 'w'), ('es', 't'), ('w', 'es')}\n",
      "condidate for merging : ('es', 't')\n",
      "word after merging: ('l', 'o', 'w', 'est', '</w>')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "__Iteration 3:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('o', 'w'), ('l', 'o'), ('w', 'est'), ('est', '</w>')}\n",
      "condidate for merging : ('est', '</w>')\n",
      "word after merging: ('l', 'o', 'w', 'est</w>')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "__Iteration 4:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('w', 'est</w>'), ('o', 'w'), ('l', 'o')}\n",
      "condidate for merging : ('l', 'o')\n",
      "word after merging: ('lo', 'w', 'est</w>')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "__Iteration 5:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('lo', 'w'), ('w', 'est</w>')}\n",
      "condidate for merging : ('lo', 'w')\n",
      "word after merging: ('low', 'est</w>')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "__Iteration 6:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('low', 'est</w>')}\n",
      "condidate for merging : ('low', 'est</w>')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "__Candidate not in BPE merges, algorithm stops.__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('low', 'est')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode('lowest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "__word split into characters:__ <tt>('l', 'o', 'w', 'i', 'n', 'g', '</w>')</tt>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Iteration 1:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('g', '</w>'), ('l', 'o'), ('i', 'n'), ('n', 'g'), ('o', 'w'), ('w', 'i')}\n",
      "condidate for merging : ('l', 'o')\n",
      "word after merging: ('lo', 'w', 'i', 'n', 'g', '</w>')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "__Iteration 2:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('g', '</w>'), ('i', 'n'), ('n', 'g'), ('lo', 'w'), ('w', 'i')}\n",
      "condidate for merging : ('lo', 'w')\n",
      "word after merging: ('low', 'i', 'n', 'g', '</w>')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "__Iteration 3:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('g', '</w>'), ('n', 'g'), ('i', 'n'), ('low', 'i')}\n",
      "condidate for merging : ('g', '</w>')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "__Candidate not in BPE merges, algorithm stops.__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('low', 'i', 'n', 'g')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode('lowing')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordPiece Tokenizer\n",
    "\n",
    "- 논문 : https://static.googleusercontent.com/media/research.google.com/ko//pubs/archive/37842.pdf\n",
    "\n",
    "- 구글이 위 WordPiece Tokenizer를 변형하여 번역기에 사용했다는 논문 : https://arxiv.org/pdf/1609.08144.pdf\n",
    "\n",
    "WordPiece Tokenizer는 BPE의 변형 알고리즘입니다. 해당 알고리즘은 병합되었을 때 코퍼스의 우도(Likelihood)를 가장 높이는 쌍을 반환합니다.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 알고리즘\n",
    "\n",
    "접두사를 추가하여 subwords를 식별하기 때문에 각 단어는 처음에 해당 접두사를 단어 내부의 모든 문자에 추가하여 분할됩니다. 예를 들어 'word'는 다음과 같이 분할됩니다.\n",
    "```python\n",
    "'w' '##o' '##r' '##d'\n",
    "```\n",
    "\n",
    "따라서 초기 알파벳에는 단어의 시작 부분에 있는 모든 문자들과 WordPiece 접두사가 선행하는 단어 내부에 있는 문자가 포함됩니다.\n",
    "\n",
    "그런 다음 BPE와 마찬가지로 WordPiece도 병합 규칙을 학습합니다. 주요 차이점은 병합할 쌍이 선택되는 방식입니다. 가장 빈번하게 출현하는 쌍을 선택하는 대신 WordPiece는 다음 공식을 사용하여 각 쌍에 대한 점수를 계산합니다.\n",
    "\n",
    "$$\\text{score} = \\frac{\\text{freq of pair}}{(\\text{freq of first element}) \\times (\\text{freq of second element})}$$\n",
    "\n",
    "쌍의 빈도는 각 부분의 빈도의 곱으로 나눔으로써, 알고리즘은 각 개별 부분들의 빈도가 낮은 쌍의 병합에 높은 우선순위를 부여합니다.\n",
    "\n",
    "예를 들어 vocabulary 내에서의 출현 빈도가 높은 ('un', '##able')쌍을 굳이 병합할 필요는 없는데, 그 이유는 'un'과 '##able'각각이 다른 단어 내에서 매무 빈번하게 출현하여 높은 빈도를 나타내기 때문입니다.\n",
    "\n",
    "반면에 'hu'와 '##ging'은 각각이 자주 사용되지 않기 때문에 ('hu', '##gging')과 같은 쌍은 아마도 더 빨리 병합될 것입니다.\n",
    "\n",
    "단어와 빈도수의 예시를 가지고 설명 하겠습니다.\n",
    "```python\n",
    "('hug', 10), ('pug', 5), ('pun', 12), ('bun', 4), ('hugs', 5)\n",
    "```\n",
    "\n",
    "분할 결과는 아래와 같습니다.\n",
    "```python\n",
    "('h', '##u', '##g', 10), ('p', '##u', '##g', 5), ('p', '##u', '##n', 12), ('b', '##u', '##n', 4), ('h', '##u', '##g', '##s', 5)\n",
    "```\n",
    "\n",
    "따라서 초기 vocabulary는 ```['b', 'h', 'p', '##g', '##n', '##s', '##u']```가 됩니다.\n",
    "\n",
    "가장 빈번한 쌍은 ('##u', '##g')(현재 20회)이지만 '##u'의 개별 빈도가 매우 높아 점수가 가장 높지는 않습니다.(1/36)\n",
    "\n",
    "'##u'가 포함된 모든 쌍은 실제로 동일한 점수를 가지므로 가장 좋은 점수는 ('##g', '##s')이 가지고 있습니다. 이는 '##u'가 없는 유일한 쌍입니다. 그리고 학습된 첫 번째 병합은 ('##g', '##s') -> ('##gs')입니다.\n",
    "\n",
    "```python\n",
    "Vocabulary : ['b', 'h', 'p', '##g', '##n', '##s', '##u', '##gs']\n",
    "Corpus : ('h', '##u', '##g', 10), ('p', '##u', '##g', 5), ('p', '##u', '##n', 12), ('b', '##u', '##n', 4), ('h', '##u', '##gs', 5)\n",
    "```\n",
    "\n",
    "이 시점에서 '##u'는 가능한 모든 쌍에 있으므로 모두 동일한 점수를 가집니다. 이 경우 첫 번째 쌍이 병합되므로 ('h', '##u') -> 'hu'가 학습됩니다.\n",
    "\n",
    "```python\n",
    "Vocabulary : ['b', 'h', 'p', '##g', '##n', '##s', '##u', '##gs', 'hu']\n",
    "Corpus : ('hu', '##g', 10), ('p', '##u', '##g', 5), ('p', '##u', '##n', 12), ('b', '##u', '##n', 4), ('hu', '##gs', 5)\n",
    "```\n",
    "이제 최고 점수는 ('hu', '##g') 및 ('hu', '##gs')가 동일하게 계산되므로 가장 큰 점수를 가진 첫 번째 쌍이 병합됩니다.\n",
    "\n",
    "```python\n",
    "Vocabulary : ['b', 'h', 'p', '##g', '##n', '##s', '##u', '##gs', 'hu', 'hug']\n",
    "Corpus : ('hug', 10), ('p', '##u', '##g', 5), ('p', '##u', '##n', 12), ('b', '##u', '##n', 4), ('hu', '##gs', 5)\n",
    "```\n",
    "\n",
    "## 토큰화 알고리즘\n",
    "\n",
    "토큰화는 WordPiece가 학습된 병합 규칙은 제외하고 최종 Vocabulary만 저장한다는 점에서 BPE와 다릅니다. 토큰화할 단어에서 시작하여 WordPiece는 Vocabulary에 있는 가장 긴 하위 단어를 찾은 다음 분할합니다.\n",
    "\n",
    "예를 들어, 위의 예에서 학습한 vocabulary를 사용하는 경우, 단어 'hugs'의 경우는 처음부터 시작하는 가장 긴 하위 단어는 vocabulary 내부에 있는 'hug'이므로 ['hug', '##s']로 분할됩니다. 그런 다음 '##s'가 vocabulary에 존재하고 이를 계속 사용할 수 있으므로 'hugs'의 토큰 결과는 ['hug', '##s']입니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordPice 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e52a2e2c8f4a018b2e9fe6b0e13355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Myeong\\anaconda3\\envs\\jin\\lib\\site-packages\\huggingface_hub\\file_download.py:127: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Myeong\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c746588be3847c68645f3ad5c96a596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6c038b3f88c420ba2f4a46bc86f8e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb88b533b21b459c83f62cf46d21a884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'BertTokenizerFast' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer\n\u001b[0;32m     10\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mbert-base-cased\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m tokenizer\u001b[39m.\u001b[39;49msave(\u001b[39m'\u001b[39m\u001b[39mC:/Users/Myeong/dding/huggingface_model/Tokenizer/bert_base_cased\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BertTokenizerFast' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'This': 3,\n",
       "             'is': 2,\n",
       "             'the': 1,\n",
       "             'Hugging': 1,\n",
       "             'Face': 1,\n",
       "             'course': 1,\n",
       "             '.': 4,\n",
       "             'chapter': 1,\n",
       "             'about': 1,\n",
       "             'tokenization': 1,\n",
       "             'section': 1,\n",
       "             'shows': 1,\n",
       "             'several': 1,\n",
       "             'tokenizer': 1,\n",
       "             'algorithms': 1,\n",
       "             'Hopefully': 1,\n",
       "             ',': 1,\n",
       "             'you': 1,\n",
       "             'will': 1,\n",
       "             'be': 1,\n",
       "             'able': 1,\n",
       "             'to': 1,\n",
       "             'understand': 1,\n",
       "             'how': 1,\n",
       "             'they': 1,\n",
       "             'are': 1,\n",
       "             'trained': 1,\n",
       "             'and': 1,\n",
       "             'generate': 1,\n",
       "             'tokens': 1})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 말뭉치에 단어 빈도 계산\n",
    "from collections import defaultdict\n",
    "\n",
    "word_freqs = defaultdict(int)\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1\n",
    "\n",
    "word_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['##a',\n",
       " '##b',\n",
       " '##c',\n",
       " '##d',\n",
       " '##e',\n",
       " '##f',\n",
       " '##g',\n",
       " '##h',\n",
       " '##i',\n",
       " '##k',\n",
       " '##l',\n",
       " '##m',\n",
       " '##n',\n",
       " '##o',\n",
       " '##p',\n",
       " '##r',\n",
       " '##s',\n",
       " '##t',\n",
       " '##u',\n",
       " '##v',\n",
       " '##w',\n",
       " '##y',\n",
       " '##z',\n",
       " ',',\n",
       " '.',\n",
       " 'F',\n",
       " 'H',\n",
       " 'T',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'w',\n",
       " 'y']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet = []\n",
    "for word in word_freqs.keys():\n",
    "    if word[0] not in alphabet:\n",
    "        alphabet.append(word[0])\n",
    "    for letter in word[1:]:\n",
    "        if f\"##{letter}\" not in alphabet:\n",
    "            alphabet.append(f\"##{letter}\")\n",
    "\n",
    "alphabet.sort()\n",
    "alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + alphabet.copy()\n",
    "\n",
    "splits = {\n",
    "    word : [c if i == 0 else f\"##{c}\" for i,c in enumerate(word)] for word in word_freqs.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T', '##h') : 0.125\n",
      "('##h', '##i') : 0.03409090909090909\n",
      "('##i', '##s') : 0.02727272727272727\n",
      "('i', '##s') : 0.1\n",
      "('t', '##h') : 0.03571428571428571\n",
      "('##h', '##e') : 0.011904761904761904\n"
     ]
    }
   ],
   "source": [
    "def compute_pair_scores(splits):\n",
    "    letter_freqs = defaultdict(int)\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            letter_freqs[split[0]] += freq\n",
    "            continue\n",
    "        for i in range(len(split)-1):\n",
    "            pair = (split[i], split[i+1])\n",
    "            letter_freqs[split[i]] += freq\n",
    "            pair_freqs[pair] += freq\n",
    "        letter_freqs[split[-1]] += freq\n",
    "    \n",
    "    scores = {\n",
    "        pair : freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]]) for pair, freq in pair_freqs.items()\n",
    "    }\n",
    "    return scores\n",
    "\n",
    "pair_scores = compute_pair_scores(splits)\n",
    "for i, key in enumerate(pair_scores.keys()):\n",
    "    print(f\"{key} : {pair_scores[key]}\")\n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', '##b') 0.2\n"
     ]
    }
   ],
   "source": [
    "best_pair = ''\n",
    "max_score = None\n",
    "for pair, score in pair_scores.items():\n",
    "    if max_score is None or max_score < score:\n",
    "        best_pair = pair\n",
    "        max_score = score\n",
    "\n",
    "print(best_pair, max_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서 학습할 첫 번째 병합은 ('a', '##b') -> 'ab'이고 vocabulary에 'ab'를 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ab', '##o', '##u', '##t']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.append('ab')\n",
    "\n",
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        i = 0\n",
    "        while i < len(split) -1:\n",
    "            if split[i] == a and split[i+1] == b:\n",
    "                merge = a + b[2:] if b.startswith(\"##\") else a + b\n",
    "                split = split[:i] + [merge] + split[i+2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits\n",
    "\n",
    "splits = merge_pair('a', '##b', splits)\n",
    "splits['about']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 70\n",
    "while len(vocab) < vocab_size:\n",
    "    scores = compute_pair_scores(splits)\n",
    "    best_pair, max_score = '', None\n",
    "    for pair, score in scores.items():\n",
    "        if max_score is None or max_score < score:\n",
    "            best_pair = pair\n",
    "            max_score = score\n",
    "    splits = merge_pair(*best_pair, splits)\n",
    "    new_token = (\n",
    "        best_pair[0] + best_pair[1][2:] if best_pair[1].startswith(\"##\") else best_pair[0] + best_pair[1]\n",
    "    )\n",
    "    vocab.append(new_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully', 'Th', '##hm', '##thm', 'Hu', 'Hug', 'Hugg', 'ch', 'cha', 'chap', 'chapt', 'sh', 'th', 'is', '##thms', '##za', '##zat', '##ut', '##ta']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hugg', '##i', '##n', '##g']\n",
      "['[UNK]']\n"
     ]
    }
   ],
   "source": [
    "def encode_word(word):\n",
    "    tokens = []\n",
    "    while len(word) > 0:\n",
    "        i = len(word)\n",
    "        while i > 0 and word[:i] not in vocab:\n",
    "            i -= 1\n",
    "        if i == 0:\n",
    "            return [\"[UNK]\"]\n",
    "        tokens.append(word[:i])\n",
    "        word = word[i:]\n",
    "        if len(word) > 0:\n",
    "            word = f\"##{word}\"\n",
    "    return tokens\n",
    "\n",
    "print(encode_word(\"Hugging\"))\n",
    "print(encode_word(\"HOgging\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Th',\n",
       " '##i',\n",
       " '##s',\n",
       " 'is',\n",
       " 'th',\n",
       " '##e',\n",
       " 'Hugg',\n",
       " '##i',\n",
       " '##n',\n",
       " '##g',\n",
       " 'Fac',\n",
       " '##e',\n",
       " 'c',\n",
       " '##o',\n",
       " '##u',\n",
       " '##r',\n",
       " '##s',\n",
       " '##e',\n",
       " '[UNK]']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "    encoded_words = [encode_word(word) for word in pre_tokenized_text]\n",
    "    return sum(encoded_words, [])\n",
    "\n",
    "tokenize('This is the Hugging Face course!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram Language Model Tokenizer\n",
    "\n",
    "- 논문 : https://arxiv.org/pdf/1804.10959.pdf\n",
    "\n",
    "유니그램 언어 모델 토크나이저는 각각의 subword들에 대해서 loss를 계산합니다. 측정된 subword들의 loss를 정렬하여, 최악의 영향을 주는 10~20%의 토큰을 제거합니다. 이를 원하는 단어 집합의 크기에 도달할 때까지 반복합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 알고리즘\n",
    "\n",
    "크기가 큰 vocabulary에서 시작하여 원하는 vocabulary 크기에 도달할 때까지 토큰을 제거합니다. \n",
    "\n",
    "학습의 각 단계에서 Unigram알고리즘은 현재 vocabulary가 주어졌을 때의 말뭉치에 대한 loss를 계산합니다. 그런 다음, vocabulary의 각 symbol에 대해, 해당 기호가 제거되면 전체 손실이 얼마나 증가할지 계산하고 가장 적게 증가하는 symbol을 찾습니다. 이렇게 찾은 symbols는 말뭉치에 대한 전체 손실에 더 적은 영향을 미치므로 어떤 의미에서는 '덜 필요(less needed)'하고 제거 대상으로 가장 적합한 후보입니다.\n",
    "\n",
    "이것은 비용이 많이 드는 작업이므로 가장 낮은 손실을 초래하는 기호 하나만 제거하지 않고, 이러한 기호들의 $p\\%$를 제거합니다.\n",
    "\n",
    "설명에는 위 예제의 말뭉치를 재사용합니다.\n",
    "```python\n",
    "('hug', 10), ('pug', 5), ('pun', 12), ('bun', 4), ('hugs', 5)\n",
    "```\n",
    "\n",
    "초기 vocabulary는 위 말뭉치에 존재하는 모든 단어들의 모든 하위 문자열(substrings)로 구성됩니다.\n",
    "```python\n",
    "['h', 'u', 'g', 'hu', 'ug', 'p', 'pu', 'n', 'un', 'b', 'bu', 's', 'hug', 'gs', 'ugs']\n",
    "```\n",
    "\n",
    "## 토큰화 알고리즘\n",
    "\n",
    "Unigram 모델은 개별 토큰들의 출현 분포가 서로 독립적(i.i.d)이라는 가정을 하는 언어 모델 유형입니다. 토큰 X의 확률이 문맥에 상관없이 동일하다는 점에서 가장 단순한 언어 모델입니다. 따라서 Unigram 언어 모델을 사용하여 텍스트를 생성하는 경우 항상 가장 일반적이고 흔한(common)토큰을 도출합니다.\n",
    "\n",
    "특정 토큰의 확률은 말뭉치 내에서의 해당 토큰 출현 빈도를 vocabulary에 존재하는 모든 토큰들의 출현 빈도의 합으로 나눈 것입니다.\n",
    "\n",
    "다음은 vocabulary에서의 모든 하위 단어(subwords)의 빈도입니다.\n",
    "```python\n",
    "(\"h\", 15) (\"u\", 36) (\"g\", 20) (\"hu\", 15) (\"ug\", 20) (\"p\", 17) (\"pu\", 17) (\"n\", 16)\n",
    "(\"un\", 16) (\"b\", 4) (\"bu\", 4) (\"s\", 5) (\"hug\", 15) (\"gs\", 5) (\"ugs\", 5)\n",
    "```\n",
    "\n",
    "따라서 모든 빈도의 합은 210이고 subword 'ug'의 확률은 20/210입니다.\n",
    "\n",
    "이제 주어진 단어를 토큰화하기 위해, 가능한 모든 토큰 분할을 살펴보고 Unigram 모델에 따라 각각의 확률을 계산합니다. 모든 토큰의 출현 빈도가 독립적인 것으로 간주되기 때문에 이 확률은 각 토큰의 확률의 곱일 뿐입니다. 예를 들어, 'pug'의 토큰화 결과인 ['p', 'u', 'g']의 확률은 다음과 같이 계산됩니다.\n",
    "\n",
    "$$P([\\text{\"p\"},\\; \\text{\"u\"},\\; \\text{\"g\"}]) = P(\\text{\"p\"}) \\times P(\\text{\"u\"}) \\times P(\\text{\"g\"}) = \\frac{5}{210} \\times \\frac{36}{210} \\times \\frac{2}{210} = 0.000389$$\n",
    "\n",
    "이에 비해 토큰화 결과인 [\"pu\", \"g\"]의 확률은 다음과 같습니다.\n",
    "\n",
    "$$P([\\text{\"pu\"},\\; \\text{\"g\"}]) = P(\\text{\"pu\"}) \\times P(\\text{\"g\"}) = \\frac{5}{210} \\times \\frac{20}{210} = 0.0022676$$\n",
    "\n",
    "따라서, [\"pu\", \"g\"]이 훨씬 더 자주 출현한다고 볼 수 있습니다. 일반적으로 가장 적은 수의 하위 토큰들로 구성된 토큰화 결과는 비교적 높은 확률을 가지며, 이는 우리가 직관적으로 원하는 결과입니다.\n",
    "\n",
    "Unigram 모델을 사용한 단어의 토큰화는 가장 높은 확률을 나타내는 분할 형태로 토큰화됩니다. 'pug'의 예에서 가능한 각 분할에 대해 얻을 수 있는 확률은 다음과 같습니다.\n",
    "```python\n",
    "['p', 'u', 'g'] : 0.000389\n",
    "['p', 'ug'] : 0.0022676\n",
    "['pu', 'g'] : 0.0022676\n",
    "```\n",
    "\n",
    "위의 경우에는 가능한 모든 분할을 찾고 확률을 계산하는 것이 쉬웠지만, 일반적으로는 더 어려울 수 있습니다. 이를 위해 사용되는 알고리즘인 **Viterbi 알고리즘**이 있습니다.\n",
    "\n",
    "기본적으로 주어진 단어에 대한 가능한 모든 분할들을 나타낼 수 있는 그래프를 구성할 수 있습니다. 그리고 만일 주어진 단어 내의 문자 a에서 b까지의 하위 단어(subword)가 vocabulary에 존재한다면, 우리는 이 그래프 내에서 a에서 출발하여 b까지 가는 그래프 내의 가지(branch)가 있다고 말할 수 있습니다. 그리고 이 하위 단어의 확률을 해당 가지(branch)에 지정할 수 있습니다.\n",
    "\n",
    "그래프에서 최고 점수를 얻을 경로를 찾기 위해 Viterbi 알고리즘은 단어 내의 각 위치에 대해 해당 위치에서 끝나는 경로의 최고 점수를 나타내는 분할을 결정합니다. 단어의 처음 위치부터 끝까지 이동하면서, 현재 위치에서 끝나는 모든 하위 단어를 검사한 다음 이 하위 단어가 시작하는 위치에서 최고의 토큰화 점수를 사용하여 최상의 점수를 찾을 수 있습니다. 그런 다음 끝에 도달하기 위해 선택한 경로를 펼치기만 하면 됩니다.\n",
    "\n",
    "```python\n",
    "Character 0 (u): \"u\" (score 0.171429)\n",
    "Character 1 (n): \"un\" (score 0.076191)\n",
    "Character 2 (h): \"un\" \"h\" (score 0.005442)\n",
    "Character 3 (u): \"un\" \"hu\" (score 0.005442)\n",
    "Character 4 (g): \"un\" \"hug\" (score 0.005442)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 알고리즘 2\n",
    "\n",
    "이제 토큰화가 어떻게 작동하는지 보았으므로 학습 과정에서 사용된 loss에 대해 조금 더 자세히 알아보겠습니다. 각각의 주어진 단계에서 이 loss는 말뭉치 내의 모든 단어를 토큰화하여 계산됩니다. 계산 과정에서 앞에서 설명한 것처럼 현재 vocabulary와 말뭉치에 있는 각 토큰의 빈도에 의해 결정된 유니그램 모델을 사용합니다.\n",
    "\n",
    "말뭉치의 각 단어별로 점수를 계산하며 손실은 해당 점수의 negative log likelihood입니다. 즉, 말뭉치에 있는 모든 단어의 $-\\log (P(\\text{word}))$합입니다.\n",
    "\n",
    "위에서 설명한 말뭉치를 가지고 설명하겠습니다.\n",
    "```python\n",
    "(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n",
    "```\n",
    "\n",
    "각 단어의 토큰화 결과 및 점수는 다음과 같습니다.\n",
    "```python\n",
    "\"hug\": [\"hug\"] (score 0.071428)\n",
    "\"pug\": [\"pu\", \"g\"] (score 0.007710)\n",
    "\"pun\": [\"pu\", \"n\"] (score 0.006168)\n",
    "\"bun\": [\"bu\", \"n\"] (score 0.001451)\n",
    "\"hugs\": [\"hug\", \"s\"] (score 0.001701)\n",
    "```\n",
    "\n",
    "이때의 loss는 다음과 같습니다.\n",
    "```python\n",
    "10 * (-log(0.071428)) + 5 * (-log(0.007710)) + 12 * (-log(0.006168)) + 4 * (-log(0.001451)) + 5 * (-log(0.001701)) = 169.8\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "556bd55ae10b4d499f70be85c59fc509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/760 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4286004ad7341208362dc41e30fb5bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ed778ca8a142e7a4a59b01aa3bede8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.38M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlnet-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'▁This': 3,\n",
       "             '▁is': 2,\n",
       "             '▁the': 1,\n",
       "             '▁Hugging': 1,\n",
       "             '▁Face': 1,\n",
       "             '▁course.': 1,\n",
       "             '▁chapter': 1,\n",
       "             '▁about': 1,\n",
       "             '▁tokenization.': 1,\n",
       "             '▁section': 1,\n",
       "             '▁shows': 1,\n",
       "             '▁several': 1,\n",
       "             '▁tokenizer': 1,\n",
       "             '▁algorithms.': 1,\n",
       "             '▁Hopefully,': 1,\n",
       "             '▁you': 1,\n",
       "             '▁will': 1,\n",
       "             '▁be': 1,\n",
       "             '▁able': 1,\n",
       "             '▁to': 1,\n",
       "             '▁understand': 1,\n",
       "             '▁how': 1,\n",
       "             '▁they': 1,\n",
       "             '▁are': 1,\n",
       "             '▁trained': 1,\n",
       "             '▁and': 1,\n",
       "             '▁generate': 1,\n",
       "             '▁tokens.': 1})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 단어의 출현 빈도 계산\n",
    "from collections import defaultdict\n",
    "\n",
    "word_freqs = defaultdict(int)\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1\n",
    "\n",
    "word_freqs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 최종적으로 원하는 크기보다 더 크게 vocabulary를 초기화해야 합니다. 초기화 과정에서 vocabulary에 모든 기본 문자들을 포함해야 합니다. 또한 길이가 더 긴 부분 문자열에 대해서는 가장 빈번하게 출현하는 것들만 추가할 것이므로 일단 빈도순으로 정렬을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁', 31),\n",
       " ('e', 21),\n",
       " ('t', 14),\n",
       " ('i', 13),\n",
       " ('s', 13),\n",
       " ('o', 13),\n",
       " ('a', 12),\n",
       " ('n', 11),\n",
       " ('h', 9),\n",
       " ('r', 9)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_freqs = defaultdict(int)\n",
    "subwords_freqs = defaultdict(int)\n",
    "for word, freq in word_freqs.items():\n",
    "    for i in range(len(word)):\n",
    "        char_freqs[word[i]] += freq\n",
    "        # 길이가 적어도 2 이상인 subword들을 추가함\n",
    "        for j in range(i+1, len(word)+1):\n",
    "            subwords_freqs[word[i:j]] += freq\n",
    "\n",
    "# Subword들을 빈도 역순으로 정렬\n",
    "sorted_subwords = sorted(subwords_freqs.items(), key=lambda x : x[1], reverse=True)\n",
    "sorted_subwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_freqs = list(char_freqs.items()) + sorted_subwords[: 300-len(char_freqs)]\n",
    "token_freqs = {token : freq for token, freq in token_freqs}\n",
    "\n",
    "# SentencePiece는 ESA(Enhanced Suffix Array)라는 보다 효율적인 알고리즘을 사용하여 초기 어휘를 생성합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로 모든 빈도의 합을 계산하여 빈도를 확률로 변환합니다. 우리 모델의 경우 확률의 로그값을 저장할 것입니다. 작은 숫자를 곱하는 것보다 로그를 더하는 것이 수치적으로 더 안정적이기 때문입니다. 이렇게 하면 모델 손실 계산이 단순화됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "total_sum = sum([freq for token, freq in token_freqs.items()])\n",
    "model = {token : -log(freq / total_sum) for token, freq in token_freqs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_word(word, model):\n",
    "    best_segmentations = [{\"start\": 0, \"score\": 1}] + [\n",
    "        {\"start\": None, \"score\": None} for _ in range(len(word))\n",
    "    ]\n",
    "    for start_idx in range(len(word)):\n",
    "        # This should be properly filled by the previous steps of the loop\n",
    "        best_score_at_start = best_segmentations[start_idx][\"score\"]\n",
    "        for end_idx in range(start_idx + 1, len(word) + 1):\n",
    "            token = word[start_idx:end_idx]\n",
    "            if token in model and best_score_at_start is not None:\n",
    "                score = model[token] + best_score_at_start\n",
    "                # If we have found a better segmentation ending at end_idx, we update\n",
    "                if (\n",
    "                    best_segmentations[end_idx][\"score\"] is None\n",
    "                    or best_segmentations[end_idx][\"score\"] > score\n",
    "                ):\n",
    "                    best_segmentations[end_idx] = {\"start\": start_idx, \"score\": score}\n",
    "\n",
    "    segmentation = best_segmentations[-1]\n",
    "    if segmentation[\"score\"] is None:\n",
    "        # We did not find a tokenization of the word -> unknown\n",
    "        return [\"<unk>\"], None\n",
    "\n",
    "    score = segmentation[\"score\"]\n",
    "    start = segmentation[\"start\"]\n",
    "    end = len(word)\n",
    "    tokens = []\n",
    "    while start != 0:\n",
    "        tokens.insert(0, word[start:end])\n",
    "        next_start = best_segmentations[start][\"start\"]\n",
    "        end = start\n",
    "        start = next_start\n",
    "    tokens.insert(0, word[start:end])\n",
    "    return tokens, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['H', 'o', 'p', 'e', 'f', 'u', 'll', 'y'], 41.19982858248713)\n",
      "(['This'], 6.2487769209879005)\n"
     ]
    }
   ],
   "source": [
    "print(encode_word('Hopefully', model))\n",
    "print(encode_word(\"This\", model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "434.111070693413"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_loss(model):\n",
    "    loss = 0\n",
    "    for word, freq in word_freqs.items():\n",
    "        _, word_loss = encode_word(word, model)\n",
    "        loss += freq * word_loss\n",
    "    return loss\n",
    "\n",
    "compute_loss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.297432184210663\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "def compute_scores(model):\n",
    "    scores = {}\n",
    "    model_loss = compute_loss(model)\n",
    "    for token, score in model.items():\n",
    "        # We always keep tokens of length 1\n",
    "        if len(token) == 1:\n",
    "            continue\n",
    "        model_without_token = copy.deepcopy(model)\n",
    "        _ = model_without_token.pop(token)\n",
    "        scores[token] = compute_loss(model_without_token) - model_loss\n",
    "    return scores\n",
    "\n",
    "scores = compute_scores(model)\n",
    "print(scores[\"ll\"])\n",
    "print(scores[\"his\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁This', '▁is', '▁the', '▁Hugging', '▁Face', '▁course.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_to_remove = 0.1\n",
    "while len(model) > 100:\n",
    "    scores = compute_scores(model)\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x : x[1])\n",
    "    # Remove percent_to_remove tokens with the lowest scores.\n",
    "    for i in range(int(len(model) * percent_to_remove)):\n",
    "        _ = token_freqs.pop(sorted_scores[i][0])\n",
    "    \n",
    "    total_sum = sum([freq for token, freq in token_freqs.items()])\n",
    "    model = {token : -log(freq / total_sum) for token, freq in token_freqs.items()}\n",
    "\n",
    "\n",
    "def tokenize(text, model):\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in words_with_offsets]\n",
    "    encoded_words = [encode_word(word, model)[0] for word in pre_tokenized_text]\n",
    "    return sum(encoded_words, [])\n",
    "\n",
    "\n",
    "tokenize('This is the Hugging Face course.', model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "96512990021685504d4683198faad895f5cd0e4b7b1aa29365fef97d0a48eb34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
