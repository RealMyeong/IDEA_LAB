{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import shape_list, BertTokenizer, TFBertModel\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from seqeval.metrics import f1_score, classification_report\n",
    "import tensorflow as tf\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ner_df = pd.read_csv('ner_train_data.csv')\n",
    "test_ner_df = pd.read_csv('ner_test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>정은 씨를 힘들게 한 가스나그, 가만둘 수 없겠죠 .</td>\n",
       "      <td>PER-B O O O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>▶ 쿠마리 한동수가 말하는 '가넷 &amp; 에르덴'</td>\n",
       "      <td>O PER-B PER-I O PER-B O PER-B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>슈나이더의 프레젠테이션은 말 청중을 위한 특별한 쇼다 .</td>\n",
       "      <td>PER-B O O CVL-B O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>지구 최대 연료탱크 수검 회사 구글이 연내 22명 안팎의 인력을 갖춘 연구개발(R&amp;...</td>\n",
       "      <td>O O TRM-B O O ORG-B DAT-B NUM-B O O O ORG-B LO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5. &lt;10:00:TI_HOUR&gt; 도이치증권대 &lt;0:1:QT_SPORTS&gt; 연예오락...</td>\n",
       "      <td>NUM-B O ORG-B O ORG-B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence                                                Tag\n",
       "0                      정은 씨를 힘들게 한 가스나그, 가만둘 수 없겠죠 .                              PER-B O O O O O O O O\n",
       "1                          ▶ 쿠마리 한동수가 말하는 '가넷 & 에르덴'                      O PER-B PER-I O PER-B O PER-B\n",
       "2                    슈나이더의 프레젠테이션은 말 청중을 위한 특별한 쇼다 .                            PER-B O O CVL-B O O O O\n",
       "3  지구 최대 연료탱크 수검 회사 구글이 연내 22명 안팎의 인력을 갖춘 연구개발(R&...  O O TRM-B O O ORG-B DAT-B NUM-B O O O ORG-B LO...\n",
       "4  5. <10:00:TI_HOUR> 도이치증권대 <0:1:QT_SPORTS> 연예오락...                              NUM-B O ORG-B O ORG-B"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ner_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>라티은-원윤정, 휘닉스파크클래식 프로골퍼</td>\n",
       "      <td>PER-B EVT-B CVL-B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5원으로 맺어진 애인까지 돈이라는 민감한 원자재를 통해 현대인의 물질만능주의를 꼬집...</td>\n",
       "      <td>NUM-B O O O O O O O O O O O FLD-B O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-날로 삼키면 맛이 어떤지 일차 드셔보시겠어요 .</td>\n",
       "      <td>O O O O NUM-B O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-네, 지었습니다 .</td>\n",
       "      <td>O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>◇신규 투자촉진에 방점=이번 접속료 조정결과에서 눈에 띄는 지점은 WCDMA/HSD...</td>\n",
       "      <td>O O O O O O O O TRM-B O TRM-B TRM-I ORG-B O TR...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence                                                Tag\n",
       "0                             라티은-원윤정, 휘닉스파크클래식 프로골퍼                                  PER-B EVT-B CVL-B\n",
       "1  5원으로 맺어진 애인까지 돈이라는 민감한 원자재를 통해 현대인의 물질만능주의를 꼬집...                NUM-B O O O O O O O O O O O FLD-B O\n",
       "2                        -날로 삼키면 맛이 어떤지 일차 드셔보시겠어요 .                                  O O O O NUM-B O O\n",
       "3                                        -네, 지었습니다 .                                              O O O\n",
       "4  ◇신규 투자촉진에 방점=이번 접속료 조정결과에서 눈에 띄는 지점은 WCDMA/HSD...  O O O O O O O O TRM-B O TRM-B TRM-I ORG-B O TR..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ner_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sentence = [sent.split() for sent in train_ner_df['Sentence'].values]\n",
    "test_data_sentence = [sent.split() for sent in test_ner_df['Sentence'].values]\n",
    "train_data_label = [tag.split() for tag in train_ner_df['Tag'].values]\n",
    "test_data_label = [tag.split() for tag in test_ner_df['Tag'].values]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['슈나이더의', '프레젠테이션은', '말', '청중을', '위한', '특별한', '쇼다', '.']\n",
      "['PER-B', 'O', 'O', 'CVL-B', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(train_data_sentence[2])\n",
    "print(train_data_label[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'PER-B', 'PER-I', 'FLD-B', 'FLD-I', 'AFW-B', 'AFW-I', 'ORG-B', 'ORG-I', 'LOC-B', 'LOC-I', 'CVL-B', 'CVL-I', 'DAT-B', 'DAT-I', 'TIM-B', 'TIM-I', 'NUM-B', 'NUM-I', 'EVT-B', 'EVT-I', 'ANM-B', 'ANM-I', 'PLT-B', 'PLT-I', 'MAT-B', 'MAT-I', 'TRM-B', 'TRM-I']\n"
     ]
    }
   ],
   "source": [
    "# 개체명 태깅 정보의 종류 확인\n",
    "labels = [label.strip() for label in open('ner_label.txt', 'r', encoding='utf=8')]\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_to_index = {tag : index for index, tag in enumerate(labels)}\n",
    "index_to_tag = {index : tag for index, tag in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'PER-B': 1, 'PER-I': 2, 'FLD-B': 3, 'FLD-I': 4, 'AFW-B': 5, 'AFW-I': 6, 'ORG-B': 7, 'ORG-I': 8, 'LOC-B': 9, 'LOC-I': 10, 'CVL-B': 11, 'CVL-I': 12, 'DAT-B': 13, 'DAT-I': 14, 'TIM-B': 15, 'TIM-I': 16, 'NUM-B': 17, 'NUM-I': 18, 'EVT-B': 19, 'EVT-I': 20, 'ANM-B': 21, 'ANM-I': 22, 'PLT-B': 23, 'PLT-I': 24, 'MAT-B': 25, 'MAT-I': 26, 'TRM-B': 27, 'TRM-I': 28}\n",
      "\n",
      "{0: 'O', 1: 'PER-B', 2: 'PER-I', 3: 'FLD-B', 4: 'FLD-I', 5: 'AFW-B', 6: 'AFW-I', 7: 'ORG-B', 8: 'ORG-I', 9: 'LOC-B', 10: 'LOC-I', 11: 'CVL-B', 12: 'CVL-I', 13: 'DAT-B', 14: 'DAT-I', 15: 'TIM-B', 16: 'TIM-I', 17: 'NUM-B', 18: 'NUM-I', 19: 'EVT-B', 20: 'EVT-I', 21: 'ANM-B', 22: 'ANM-I', 23: 'PLT-B', 24: 'PLT-I', 25: 'MAT-B', 26: 'MAT-I', 27: 'TRM-B', 28: 'TRM-I'}\n"
     ]
    }
   ],
   "source": [
    "print(tag_to_index)\n",
    "print()\n",
    "print(index_to_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "tag_size = len(tag_to_index)\n",
    "print(tag_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 전처리 예시  \n",
    "\n",
    "임의로 훈련 데이터 중 1번 인덱스의 문장과 레이블을 선택하고 이에 대해서 전처리를 진행해보겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▶', '쿠마리', '한동수가', '말하는', \"'가넷\", '&', \"에르덴'\"]\n",
      "['O', 'PER-B', 'PER-I', 'O', 'PER-B', 'O', 'PER-B']\n",
      "[0, 1, 2, 0, 1, 0, 1]\n",
      "7\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('klue/bert-base')\n",
    "\n",
    "sent = train_data_sentence[1]\n",
    "label = train_data_label[1]\n",
    "\n",
    "print(sent)\n",
    "print(label)\n",
    "print([tag_to_index[idx] for idx in label])\n",
    "print(len(sent))\n",
    "print(len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT 토크나이저 적용 후 문장 :  ['▶', '쿠', '##마리', '한동', '##수', '##가', '말', '##하', '##는', \"'\", '가', '##넷', '&', '에르', '##덴', \"'\"]\n",
      "레이블 :  ['O', 'PER-B', 'PER-I', 'O', 'PER-B', 'O', 'PER-B']\n",
      "레이블의 정수 인코딩 :  [0, 1, 2, 0, 1, 0, 1]\n",
      "문장의 길이 : 16\n",
      "레이블의 길이 : 7\n"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "\n",
    "for one_word in sent:\n",
    "    # 각 단어에 대해 subword로 분리\n",
    "    # one_word = '쿠마리' ==> subword_tokens = ['쿠', '##마리']\n",
    "    subword_tokens = tokenizer.tokenize(one_word)\n",
    "    tokens.extend(subword_tokens)\n",
    "\n",
    "print('BERT 토크나이저 적용 후 문장 : ', tokens)\n",
    "print('레이블 : ', label)\n",
    "print('레이블의 정수 인코딩 : ', [tag_to_index[idx] for idx in label])\n",
    "print('문장의 길이 :', len(tokens))\n",
    "print('레이블의 길이 :', len(label))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 결과를 보면 '쿠마리'가 '쿠'와 '##마리'로 분리되는 등 단어들이 subword로 분리되었습니다. 이제 문장의 길이가 길어지면서 레이블의 길이와 달라지게 됩니다. 따라서 레이블의 길이도 문장의 길이와 일치하도록 추가적인 처리를 진행해야 합니다. '쿠마리'의 레이블은 'PER-B'였습니다. 그렇다면 '쿠'와 '##마리'의 레이블은 어떻게 해야할까요?\n",
    "<br />\n",
    "\n",
    "이 경우 첫 번째 subword에 대해서만 기존의 레이블을 부여하고 뒤에 생겨난 subwords에 대해서는 레이블을 주지 않는 방법이 있습니다. 따라서 첫 번째 서브워드가 아닌 경우에 정수 -100을 부여하는 방식을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화 후 문장 :  ['▶', '쿠', '##마리', '한동', '##수', '##가', '말', '##하', '##는', \"'\", '가', '##넷', '&', '에르', '##덴', \"'\"]\n",
      "레이블 :  ['O', 'PER-B', '[PAD]', 'PER-I', '[PAD]', '[PAD]', 'O', '[PAD]', '[PAD]', 'PER-B', '[PAD]', '[PAD]', 'O', 'PER-B', '[PAD]', '[PAD]']\n",
      "레이블의 정수 인코딩 :  [0, 1, -100, 2, -100, -100, 0, -100, -100, 1, -100, -100, 0, 1, -100, -100]\n",
      "문장의 길이 : 16\n",
      "레이블의 길이 :  16\n"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "labels_ids = []\n",
    "for one_word, label_token in zip(train_data_sentence[1], train_data_label[1]):\n",
    "    subword_tokens = tokenizer.tokenize(one_word)\n",
    "    tokens.extend(subword_tokens)\n",
    "    labels_ids.extend([tag_to_index[label_token]] + [-100] * (len(subword_tokens)-1))\n",
    "\n",
    "print('토큰화 후 문장 : ', tokens)\n",
    "print('레이블 : ', ['[PAD]' if idx==-100 else index_to_tag[idx] for idx in labels_ids])\n",
    "print('레이블의 정수 인코딩 : ', labels_ids)\n",
    "print('문장의 길이 :', len(tokens))\n",
    "print('레이블의 길이 : ', len(labels_ids))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 결과에서 '레이블의 정수 인코딩'결과를 보면 '쿠'에 대해서는 PER-B에 해당하는 정수 1을 부여하였지만, '##마리'에 대해서는 -100을 부여하였습니다. 마찬가지로 기존 '한동수가'에 PER-I가 부여되어 있었으나, '한동'에는 PER-I에 해당하는 정수인 2를 부여하지만 그 뒤의 서브워드들인 '##수', '##가'에 대해서는 -100을 부여합니다.\n",
    "<br />\n",
    "\n",
    "-100을 부여하고나서는 실질적으로 학습할 때는 해당 레이블에 대해서는 학습을 무시하는 방법을 사용합니다. 정확히는 -100을 레이블에서는 패딩 토큰[PAD]로 사용합니다. 레이블에 대해서 문장의 길이를 맞추는 패딩을 진행할 때도 -100을 사용하겠습니다. 그리고 이후 손실 함수에서 -100을 무시하도록 추가 처리를 진행하겠습니다.\n",
    "<br />\n",
    "\n",
    "이제 위 과정을 한 번에 진행하고, 어텐션 마스크까지 생성하는 함수를 정의하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, labels, max_seq_len, tokenizer, pad_token_id_for_segment=0, pad_token_id_for_label=-100):\n",
    "    cls_token = tokenizer.cls_token\n",
    "    sep_token = tokenizer.sep_token\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    input_ids, attention_masks, token_type_ids, data_labels = [], [], [], []\n",
    "\n",
    "    for example, label in tqdm(zip(examples, labels), total=len(examples)):\n",
    "        tokens = []\n",
    "        labels_ids = []\n",
    "        for one_word, label_token in zip(example, label):\n",
    "            subword_tokens = tokenizer.tokenize(one_word)\n",
    "            tokens.extend(subword_tokens)\n",
    "\n",
    "            labels_ids.extend([tag_to_index[label_token]] + [pad_token_id_for_label] * (len(subword_tokens) - 1))\n",
    "\n",
    "        \n",
    "        special_tokens_count = 2\n",
    "        if len(tokens) > max_seq_len - special_tokens_count:\n",
    "            tokens = tokens[:(max_seq_len - special_tokens_count)]\n",
    "            labels_ids = labels_ids[:(max_seq_len - special_tokens_count)]\n",
    "        \n",
    "        tokens += [sep_token]\n",
    "        labels_ids += [pad_token_id_for_label]\n",
    "\n",
    "        tokens = [cls_token] + tokens\n",
    "        labels_ids = [pad_token_id_for_label] + labels_ids\n",
    "\n",
    "        input_id = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        attention_mask = [1] * len(input_id)\n",
    "\n",
    "        padding_count = max_seq_len - len(input_id)\n",
    "\n",
    "        input_id = input_id + ([pad_token_id] * padding_count)\n",
    "        attention_mask = attention_mask + ([0] * padding_count)\n",
    "\n",
    "        token_type_id = [pad_token_id_for_segment] * max_seq_len\n",
    "\n",
    "        label = labels_ids + ([pad_token_id_for_label] * padding_count)\n",
    "        \n",
    "        assert len(input_id) == max_seq_len, \"ERROR with input length {} vs {}\".format(len(input_id), max_seq_len)\n",
    "        assert len(attention_mask) == max_seq_len, \"ERROR with attention mask length {} vs {}\".format(len(attention_mask), max_seq_len)\n",
    "        assert len(token_type_id) == max_seq_len, \"ERROR with token type length {} vs {}\".format(len(token_type_id), max_seq_len)\n",
    "        assert len(label) == max_seq_len, \"ERROR with labels length {} vs {}\".format(len(label), max_seq_len)\n",
    "\n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        data_labels.append(label)\n",
    "    \n",
    "    input_ids = np.array(input_ids, dtype=int)\n",
    "    attention_masks = np.array(attention_masks, dtype=int)\n",
    "    token_type_ids = np.array(token_type_ids, dtype=int)\n",
    "    data_labels = np.asarray(data_labels, dtype=np.int32)\n",
    "\n",
    "    return (input_ids, attention_masks, token_type_ids), data_labels\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81000/81000 [00:26<00:00, 3041.38it/s]\n",
      "100%|██████████| 9000/9000 [00:02<00:00, 3073.20it/s]\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = convert_examples_to_features(train_data_sentence, train_data_label, max_seq_len=128, tokenizer=tokenizer)\n",
    "x_test, y_test = convert_examples_to_features(test_data_sentence, test_data_label, max_seq_len=128, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 :  ['정은', '씨를', '힘들게', '한', '가스나그,', '가만둘', '수', '없겠죠', '.']\n",
      "기존 레이블 :  ['PER-B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "--------------------------------------------------\n",
      "토큰화 후 원문 :  ['[CLS]', '정은', '씨', '##를', '힘들', '##게', '한', '가스', '##나', '##그', ',', '가만', '##둘', '수', '없', '##겠', '##죠', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "토큰화 후 레이블 :  ['[PAD]', 'PER-B', 'O', '[PAD]', 'O', '[PAD]', 'O', 'O', '[PAD]', '[PAD]', '[PAD]', 'O', '[PAD]', 'O', 'O', '[PAD]', '[PAD]', 'O', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "--------------------------------------------------\n",
      "정수 인코딩 결과 :  [    2 17915  1370  2138  4390  2318  1891  5809  2075  2029    16  6836\n",
      "  3056  1295  1415  2918  2321    18     3     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "정수 인코딩 레이블 :  [-100    1    0 -100    0 -100    0    0 -100 -100 -100    0 -100    0\n",
      "    0 -100 -100    0 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100]\n"
     ]
    }
   ],
   "source": [
    "print('원문 : ',train_data_sentence[0])\n",
    "print('기존 레이블 : ',train_data_label[0])\n",
    "print('-'*50)\n",
    "print('토큰화 후 원문 : ', [tokenizer.decode([word]) for word in x_train[0][0]])\n",
    "print('토큰화 후 레이블 : ', ['[PAD]' if idx==-100 else index_to_tag[idx] for idx in y_train[0]])\n",
    "print('-'*50)\n",
    "print('정수 인코딩 결과 : ', x_train[0][0])\n",
    "print('정수 인코딩 레이블 : ', y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment encoding :  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Attention mask :  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print('Segment encoding : ', x_train[2][0])\n",
    "print('Attention mask : ', x_train[1][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델링\n",
    "<br />\n",
    "\n",
    "이번에는 모든 입력에 대해서 출력을 수행해야 하는 Many-to-Many 문제에 해당하므로 outputs[0]을 사용하여 문제를 풀어야 합니다.\n",
    "<br />\n",
    "\n",
    "outputs[0]과 연결되는 출력층에는 레이블의 개수인 num_labels를 전달합니다. 이번에는 다중 클래스 분류 문제임에도 출력층에 소프트맥스 함수를 사용하지 않았는데, 이는 출력층에서 소프트맥스를 사용하지 않고 손실 함수에서 이를 처리하도록 하는 구현 방식을 사용하기 위함입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFBertForTokenClassification(tf.keras.Model):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super(TFBertForTokenClassification, self).__init__()\n",
    "        self.bert = TFBertModel.from_pretrained(model_name, from_pt=True)\n",
    "        self.classifier = tf.keras.layers.Dense(num_labels, kernel_initializer=tf.keras.initializers.TruncatedNormal(0.02), name='classifier')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        input_ids, attention_mask, token_type_ids = inputs\n",
    "        outputs = self.bert(input_ids = input_ids, attention_mask=attention_mask, token_type_ids = token_type_ids)\n",
    "\n",
    "        all_output = outputs[0]\n",
    "        prediction = self.classifier(all_output)\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 함수\n",
    "def compute_loss(labels, logits):\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "    # -100의 값을 가진 정수에 대해서는 오차를 반영하지 않도록 labels를 수정\n",
    "    active_loss = tf.reshape(labels, (-1,)) != -100\n",
    "\n",
    "    # active_loss로부터 reduced_logits와 labels를 각가 얻는다.\n",
    "    reduced_logits = tf.boolean_mask(tf.reshape(logits, (-1, shape_list(logits)[2])), active_loss)\n",
    "    labels = tf.boolean_mask(tf.reshape(labels, (-1,)), active_loss)\n",
    "\n",
    "    return loss_fn(labels, reduced_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.embeddings.position_ids', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertForTokenClassification('klue/bert-base', num_labels=tag_size)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(optimizer=optimizer, loss=compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1score(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, x_test, y_test):\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "    \n",
    "    def sequences_to_tags(self, label_ids, pred_ids):\n",
    "        label_list = []\n",
    "        pred_list = []\n",
    "\n",
    "        for i in range(0, len(label_ids)):\n",
    "            label_tag = []\n",
    "            pred_tag = []\n",
    "\n",
    "            for label_index, pred_index in zip(label_ids[i], pred_ids[i]):\n",
    "                if label_index != -100:\n",
    "                    label_tag.append(index_to_tag[label_index])\n",
    "                    pred_tag.append(index_to_tag[pred_index])\n",
    "            \n",
    "            label_list.append(label_tag)\n",
    "            pred_list.append(pred_tag)\n",
    "        \n",
    "        return label_list, pred_list\n",
    "    \n",
    "    # 에포크가 끝날 때마다 실행되는 함수\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "\n",
    "        y_predicted = self.model.predict(self.x_test)\n",
    "        y_predicted = np.argmax(y_predicted, axis=2)\n",
    "\n",
    "        label_list, pred_list = self.sequences_to_tags(self.y_test, y_predicted)\n",
    "\n",
    "        score = f1_score(label_list, pred_list, suffix=True)\n",
    "        print(' - f1: {:4.2f}'.format(score * 100))\n",
    "        print(classification_report(label_list, pred_list, suffix=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "282/282 [==============================] - 25s 80ms/step\n",
      " - f1: 84.65\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AFW       0.60      0.57      0.59       394\n",
      "         ANM       0.75      0.73      0.74       701\n",
      "         CVL       0.85      0.79      0.82      5758\n",
      "         DAT       0.92      0.91      0.91      2521\n",
      "         EVT       0.75      0.76      0.75      1094\n",
      "         FLD       0.70      0.33      0.45       228\n",
      "         LOC       0.84      0.85      0.84      2126\n",
      "         MAT       0.50      0.08      0.14        12\n",
      "         NUM       0.91      0.92      0.91      5590\n",
      "         ORG       0.87      0.86      0.87      4086\n",
      "         PER       0.89      0.87      0.88      4426\n",
      "         PLT       0.57      0.12      0.20        34\n",
      "         TIM       0.87      0.83      0.85       314\n",
      "         TRM       0.76      0.64      0.70      1964\n",
      "\n",
      "   micro avg       0.86      0.83      0.85     29248\n",
      "   macro avg       0.77      0.66      0.69     29248\n",
      "weighted avg       0.86      0.83      0.84     29248\n",
      "\n",
      "2532/2532 [==============================] - 614s 237ms/step - loss: 0.2708\n",
      "Epoch 2/3\n",
      "282/282 [==============================] - 22s 79ms/step\n",
      " - f1: 86.24\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AFW       0.64      0.63      0.64       394\n",
      "         ANM       0.77      0.74      0.76       701\n",
      "         CVL       0.85      0.84      0.84      5758\n",
      "         DAT       0.92      0.93      0.92      2521\n",
      "         EVT       0.76      0.78      0.77      1094\n",
      "         FLD       0.68      0.64      0.66       228\n",
      "         LOC       0.86      0.87      0.86      2126\n",
      "         MAT       0.17      0.08      0.11        12\n",
      "         NUM       0.91      0.93      0.92      5590\n",
      "         ORG       0.89      0.87      0.88      4086\n",
      "         PER       0.89      0.90      0.90      4426\n",
      "         PLT       0.57      0.12      0.20        34\n",
      "         TIM       0.78      0.90      0.84       314\n",
      "         TRM       0.73      0.75      0.74      1964\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     29248\n",
      "   macro avg       0.74      0.71      0.72     29248\n",
      "weighted avg       0.86      0.86      0.86     29248\n",
      "\n",
      "2532/2532 [==============================] - 594s 234ms/step - loss: 0.1516\n",
      "Epoch 3/3\n",
      "282/282 [==============================] - 22s 79ms/step\n",
      " - f1: 85.87\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AFW       0.65      0.65      0.65       394\n",
      "         ANM       0.76      0.75      0.75       701\n",
      "         CVL       0.84      0.85      0.84      5758\n",
      "         DAT       0.91      0.93      0.92      2521\n",
      "         EVT       0.74      0.79      0.77      1094\n",
      "         FLD       0.54      0.71      0.61       228\n",
      "         LOC       0.86      0.85      0.86      2126\n",
      "         MAT       0.25      0.33      0.29        12\n",
      "         NUM       0.90      0.93      0.91      5590\n",
      "         ORG       0.86      0.88      0.87      4086\n",
      "         PER       0.90      0.89      0.89      4426\n",
      "         PLT       0.43      0.26      0.33        34\n",
      "         TIM       0.87      0.89      0.88       314\n",
      "         TRM       0.73      0.76      0.75      1964\n",
      "\n",
      "   micro avg       0.85      0.87      0.86     29248\n",
      "   macro avg       0.73      0.75      0.74     29248\n",
      "weighted avg       0.85      0.87      0.86     29248\n",
      "\n",
      "2532/2532 [==============================] - 589s 233ms/step - loss: 0.0988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29453739490>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score_report = F1score(x_test, y_test)\n",
    "\n",
    "model.fit(x_train, y_train, epochs=3, batch_size=32, callbacks=[f1_score_report])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 및 평가\n",
    "def convert_examples_to_features_for_prediction(examples, max_seq_len, tokenizer, pad_token_id_for_segment=0, pad_token_id_for_label=-100):\n",
    "    cls_token = tokenizer.cls_token\n",
    "    sep_token = tokenizer.sep_token\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    input_ids, attention_masks, token_type_ids, label_masks=[], [], [], []\n",
    "\n",
    "    for example in tqdm(examples):\n",
    "        tokens = []\n",
    "        label_mask = []\n",
    "        for one_word in example:\n",
    "            subword_tokens = tokenizer.tokenize(one_word)\n",
    "            tokens.extend(subword_tokens)\n",
    "            label_mask.extend([0] + [pad_token_id_for_label] * (len(subword_tokens)-1))\n",
    "\n",
    "        special_tokens_count = 2\n",
    "        if len(tokens) > max_seq_len - special_tokens_count:\n",
    "            tokens = tokens[:(max_seq_len - special_tokens_count)]\n",
    "            label_mask = label_mask[:(max_seq_len - special_tokens_count)]\n",
    "\n",
    "        tokens += [sep_token]\n",
    "        label_mask += [pad_token_id_for_label]\n",
    "\n",
    "        tokens = [cls_token] + tokens\n",
    "        label_mask = [pad_token_id_for_label] + label_mask\n",
    "\n",
    "        input_id = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        attention_mask = [1] * len(input_id)\n",
    "        padding_count = max_seq_len - len(input_id)\n",
    "        input_id = input_id + ([pad_token_id] * padding_count)\n",
    "        attention_mask = attention_mask + ([0] * padding_count)\n",
    "        token_type_id = [pad_token_id_for_segment] * max_seq_len\n",
    "        label_mask = label_mask + ([pad_token_id_for_label] * padding_count)\n",
    "\n",
    "        assert len(input_id) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_id), max_seq_len)\n",
    "        assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(len(attention_mask), max_seq_len)\n",
    "        assert len(token_type_id) == max_seq_len, \"Error with token type length {} vs {}\".format(len(token_type_id), max_seq_len)\n",
    "        assert len(label_mask) == max_seq_len, \"Error with labels length {} vs {}\".format(len(label_mask), max_seq_len)\n",
    "\n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        label_masks.append(label_mask)\n",
    "\n",
    "    input_ids = np.array(input_ids, dtype=int)\n",
    "    attention_masks = np.array(attention_masks, dtype=int)\n",
    "    token_type_ids = np.array(token_type_ids, dtype=int)\n",
    "    label_masks = np.asarray(label_masks, dtype=np.int32)\n",
    "\n",
    "    return (input_ids, attention_masks, token_type_ids), label_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 2506.16it/s]\n"
     ]
    }
   ],
   "source": [
    "x_pred, label_masks = convert_examples_to_features_for_prediction(test_data_sentence[:5], max_seq_len=128, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_prediction(examples, max_seq_len, tokenizer):\n",
    "    examples = [sent.split() for sent in examples]\n",
    "    x_pred, label_masks = convert_examples_to_features_for_prediction(examples, max_seq_len=128, tokenizer=tokenizer)\n",
    "    y_predicted = model.predict(x_pred)\n",
    "    y_predicted = np.argmax(y_predicted, axis=2)\n",
    "\n",
    "    pred_list = []\n",
    "    result_list = []\n",
    "\n",
    "    for i in range(0, len(label_masks)):\n",
    "        pred_tag = []\n",
    "        for label_index, pred_index in zip(label_masks[i], y_predicted[i]):\n",
    "            if label_index != -100:\n",
    "                pred_tag.append(index_to_tag[pred_index])\n",
    "        pred_list.append(pred_tag)\n",
    "\n",
    "    for example, pred in zip(examples, pred_list):\n",
    "        one_sample_result = []\n",
    "        for one_word, label_token in zip(example, pred):\n",
    "            one_sample_result.append((one_word, label_token))\n",
    "        result_list.append(one_sample_result)\n",
    "    \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = '오리온스는 리그 최정상급 포인트가드 김동훈을 앞세우는 빠른 공수전환이 돋보이는 팀이다'\n",
    "sent2 = '하이신사에 속한 섬들도 위로 솟아 있는데 타인은 살고 있어요'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 3126.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 185ms/step\n"
     ]
    }
   ],
   "source": [
    "test_sample = [sent1, sent2]\n",
    "\n",
    "result_list = ner_prediction(test_sample, max_seq_len=128, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('오리온스는', 'ORG-B'),\n",
       "  ('리그', 'O'),\n",
       "  ('최정상급', 'O'),\n",
       "  ('포인트가드', 'CVL-B'),\n",
       "  ('김동훈을', 'PER-B'),\n",
       "  ('앞세우는', 'O'),\n",
       "  ('빠른', 'O'),\n",
       "  ('공수전환이', 'O'),\n",
       "  ('돋보이는', 'O'),\n",
       "  ('팀이다', 'O')],\n",
       " [('하이신사에', 'LOC-B'),\n",
       "  ('속한', 'O'),\n",
       "  ('섬들도', 'O'),\n",
       "  ('위로', 'O'),\n",
       "  ('솟아', 'O'),\n",
       "  ('있는데', 'O'),\n",
       "  ('타인은', 'O'),\n",
       "  ('살고', 'O'),\n",
       "  ('있어요', 'O')]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
