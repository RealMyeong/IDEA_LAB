{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BertWordPiece Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8112052</td>\n",
       "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8132799</td>\n",
       "      <td>디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4655635</td>\n",
       "      <td>폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9251303</td>\n",
       "      <td>와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10067386</td>\n",
       "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>8963373</td>\n",
       "      <td>포켓 몬스터 짜가 ㅡㅡ;;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>3302770</td>\n",
       "      <td>쓰.레.기</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>5458175</td>\n",
       "      <td>완전 사이코영화. 마지막은 더욱더 이 영화의질을 떨어트린다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>6908648</td>\n",
       "      <td>왜난 재미없었지 ㅠㅠ 라따뚜이 보고나서 스머프 봐서 그런가 ㅋㅋ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>8548411</td>\n",
       "      <td>포풍저그가나가신다영차영차영차</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                           document  label\n",
       "0        8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
       "1        8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n",
       "2        4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1\n",
       "3        9251303  와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...      1\n",
       "4       10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1\n",
       "...          ...                                                ...    ...\n",
       "199995   8963373                                     포켓 몬스터 짜가 ㅡㅡ;;      0\n",
       "199996   3302770                                              쓰.레.기      0\n",
       "199997   5458175                  완전 사이코영화. 마지막은 더욱더 이 영화의질을 떨어트린다.      0\n",
       "199998   6908648                왜난 재미없었지 ㅠㅠ 라따뚜이 보고나서 스머프 봐서 그런가 ㅋㅋ      0\n",
       "199999   8548411                                    포풍저그가나가신다영차영차영차      0\n",
       "\n",
       "[200000 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "naver_df = pd.read_table('C:/Users/Myeong/dding/data/딥러닝-자연어처리입문/ratings.txt')\n",
    "naver_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "naver_df.dropna(how='any', inplace=True)\n",
    "with open('naver_review.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(naver_df['document']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase : 대소문자 구분 여부. True일 경우 구분하지 않음\n",
    "# strip_accents : True일 경우 악센트 제거\n",
    "tokenizer = BertWordPieceTokenizer(lowercase=False, strip_accents=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- files : 단어 집합을 얻기 위해 학습할 데이터\n",
    "\n",
    "- vocab_size : 단어 집합의 크기\n",
    "\n",
    "- limit_alphabet : 병합 전의 초기 토큰의 허용 개수.\n",
    "\n",
    "- min_frequency : 최소 해당 횟수만큼 등장한 쌍(pair)의 경우에만 병합 대상이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'naver_review.txt'\n",
    "vocab_size = 32000\n",
    "limit_alphabet = 6000\n",
    "min_frequency = 5\n",
    "\n",
    "tokenizer.train(files=data_file, vocab_size=vocab_size, limit_alphabet=limit_alphabet, min_frequency=min_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./vocab.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab 저장\n",
    "tokenizer.save_model('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[PAD]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[UNK]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[CLS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[MASK]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31995</th>\n",
       "      <td>##었니</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31996</th>\n",
       "      <td>##어질</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31997</th>\n",
       "      <td>##어찌</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31998</th>\n",
       "      <td>##도우</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31999</th>\n",
       "      <td>##도에서</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0       [PAD]\n",
       "1       [UNK]\n",
       "2       [CLS]\n",
       "3       [SEP]\n",
       "4      [MASK]\n",
       "...       ...\n",
       "31995    ##었니\n",
       "31996    ##어질\n",
       "31997    ##어찌\n",
       "31998    ##도우\n",
       "31999   ##도에서\n",
       "\n",
       "[32000 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab load\n",
    "df = pd.read_fwf('vocab.txt', header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized :  ['안녕하세요', '경', '##상', '##국', '##립', '##대', '##학교', '진', '##명', '##입니다', '.']\n",
      "\n",
      "word to int :  [24358, 639, 3296, 3349, 3730, 3257, 7855, 2514, 3749, 5922, 18]\n",
      "\n",
      "decoding :  안녕하세요 경상국립대학교 진명입니다.\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode('안녕하세요 경상국립대학교 진명입니다.')\n",
    "print('tokenized : ', encoded.tokens)\n",
    "print()\n",
    "print('word to int : ', encoded.ids)\n",
    "print()\n",
    "print('decoding : ', tokenizer.decode(encoded.ids))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기타 토크나이저\n",
    "\n",
    "- CharBPETokenizer : 오리지널 BPE\n",
    "\n",
    "- ByteLevelBPETokenizer : BPE의 바이트 버전\n",
    "\n",
    "- SentencePieceBPETokenizer : SentencePiece와 호환되는 BPE 구현체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁안', '녕', '하세요', '▁경', '상', '국', '립', '대', '학교', '▁진', '명', '입니다.']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer, CharBPETokenizer, SentencePieceBPETokenizer\n",
    "\n",
    "tokenizer = SentencePieceBPETokenizer()\n",
    "tokenizer.train('naver_review.txt', vocab_size=10000, min_frequency=5)\n",
    "\n",
    "sample_text = '안녕하세요 경상국립대학교 진명입니다.'\n",
    "\n",
    "encoded = tokenizer.encode(sample_text)\n",
    "print(encoded.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ìķĪë', 'ħķ', 'íķĺìĦ¸ìļĶ', 'Ġê²½', 'ìĥģ', 'êµŃ', 'ë¦½', 'ëĮĢ', 'íķĻêµĲ', 'Ġì§Ħ', 'ëªħ', 'ìŀħëĭĪëĭ¤', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train('naver_review.txt', vocab_size=10000, min_frequency=5)\n",
    "\n",
    "encoded = tokenizer.encode(sample_text)\n",
    "print(encoded.tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "96512990021685504d4683198faad895f5cd0e4b7b1aa29365fef97d0a48eb34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
