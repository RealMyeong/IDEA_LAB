{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 사전 훈련된 워드 임베딩\n",
    "\n",
    "앞서 Word2Vec, FastText, GloVe와 같은 워드 임베딩 방법론들을 설명했습니다. 어떤 태스크를 수행할 때, 임베딩을 사용하는 방법으로는 크게 두 가지가 있습니다. \n",
    "<br />\n",
    "\n",
    "\n",
    "1. 임베딩 층(Embedding Layer)을 랜덤 초기화하여 처음부터 학습하는 방법\n",
    "\n",
    "2. 방대한 데이터로 Word2Vec 등과 같은 임베딩 알고리즘으로 사전에 학습된 임베딩 벡터들을 가져와 사용하는 방법\n",
    "<br />\n",
    "\n",
    "하지만 위 두 방법은 모두 하나의 단어가 하나의 벡터값으로 매핑되므로, 문맥을 고려하지 못 하여 다의어나 동음이의어를 구분하지 못하는 문제점이 있습니다. 예를들어 '사과'라는 단어는 먹는 '사과'도 있지만, 용서를 빈다는 의미로도 쓰이기 때문입니다.\n",
    "<br />\n",
    "\n",
    "이러한 한계점은 사전 훈련된 언어 모델을 사용하므로서 극복할 수 있었으며 아래에서 언급할 ELMo나 BERT 등이 이러한 문제의 해결책입니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pretrained Language Model\n",
    "\n",
    "### Semi-Supervised Sequence Learning, Google, 2015\n",
    "\n",
    "![](https://wikidocs.net/images/page/108730/image1.PNG)\n",
    "2015년 구글은 'Semi-supervised Sequence Learning'이라는 논문에서 LSTM 언어 모델을 학습하고나서 이렇게 학습한 LSTM을 텍스트 분류에 추가 학습하는 방법을 보였습니다. 이 방법은 우선 LSTM 언어 모델을 학습합니다. 언어 모델은 주어진 텍스트에서 이전 단어들로부터 다음 단어를 예측하도록 학습하므로 기본적으로 별도의 레이블이 부착되지 않은 텍스트 데이터로도 학습 가능합니다.\n",
    "<br />\n",
    "\n",
    "사전 훈련된 워드 임베딩과 마찬가지로 사전 훈련된 언어 모델의 강점은 학습 전 사람이 별도 레이블을 지정해줄 필요가 없다는 점입니다. 그리고 이렇게 레이블이 없는 데이터로 학습된 LSTM과 가중치가 랜덤으로 초기화 된 LSTM 두 가지를 두고, 텍스트 분류와 같은 문제를 학습하여 사전 훈련된 언어 모델을 사용한 전자의 경우가 더 좋은 성능을 얻을 수 있다는 가능성을 보였습니다.\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "### ELMo : Deep Contextual Word Embedding\n",
    "\n",
    "![](https://wikidocs.net/images/page/108730/image2.PNG)\n",
    "\n",
    "ELMo는 순방향 언어 모델과 역방향 언어 모델을 각각 따로 학습시킨 후에, 이렇게 사전 학습된 언어 모델로부터 임베딩 값을 얻는다는 아이디어였습니다. 이러한 임베딩은 문맥에 따라서 임베딩 벡터값이 달라지므로, 기존 워드 임베딩인 Word2Vec나 GloVe 등의 다의어를 구분할 수 없었던 문제점을 해결할 수 있었습니다. \n",
    "<br />\n",
    "\n",
    "이어서 언어 모델은 RNN계열의 신경망에서 탈피하기 시작합니다. 트랜스포머가 번역기와 같은 인코더-디코더 구조에서 LSTM을 뛰어넘는 좋은 성능을 얻자, LSTM이 아닌 트랜스포머로 사전 훈련된 언어 모델을 학습하는 시도가 등장했습니다.\n",
    "<br />\n",
    "\n",
    "### Improving Language Understanding by Generative Pre-training\n",
    "<br />\n",
    "\n",
    "![](https://wikidocs.net/images/page/108730/image3.PNG)\n",
    "트랜스포머의 디코더는 LSTM 언어 모델처럼 순차적으로 이전 단어들로부터 다음 단어를 예측합니다. Open AI는 트랜스포머 디코더로 총 12개의 층을 쌓은 후에 방대한 텍스트 데이터를 학습시킨 언어 모델 GPT-1을 만들었습니다. Open AI는 GPT-1에 여러 다양한 태스크를 위해 추가 학습을 진행하였을 때, 다양한 태스크에서 높은 성능을 얻을 수 있음을 입증했습니다. \n",
    "<br />\n",
    "\n",
    "\n",
    "![](https://wikidocs.net/images/page/108730/image4.PNG)\n",
    "<br />\n",
    "\n",
    "위의 좌측 그림에 있는 단방향 언어모델은 지금까지 배운 전형적인 언어 모델입니다. 시작 토큰이 들어오면, 다음 단어를 예측하고 그리고 그 다음 단어를 예측합니다.\n",
    "<br />\n",
    "\n",
    "반면, 우측에 있는 양방향 언어 모델은 지금까지 본 적 없던 형태의 언어 모델입니다. 언어의 문맥이라는 것은 실제로는 양방향입니다. 텍스트 분류나 개체명 인식 등에서 양방향 LSTM을 사용하여 모델을 구현해서 좋은 성능을 얻을 수 있었던 것을 상기해봅시다. 하지만 이전 단어들로부터 다음 단어를 예측하는 언어 모델의 특정으로 인해 위의 그림과 같은 양방향 언어 모델을 사용할 수 없으므로, 그 대안으로 ELMo에서는 순방향과 역방향이라는 두 개의 단방향 언어 모델을 따로 준비하여 학습하는 방법을 사용했습니다.\n",
    "<br />\n",
    "\n",
    "이처럼 기존 언어 모델로는 양방향 구조를 도입할 수 없으므로, 양방향 구조를 도입하기 위해서 새로운 구조의 언어 모델이 탄생했는데 바로 마스크드 언어 모델(Masked Language Model)입니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Masked Language Model\n",
    "\n",
    "마스크드 언어 모델은 입력 텍스트의 단어 집합의 15%의 단어를 랜덤으로 마스킹(Masking)합니다. 여기서 마스킹이란 원래의 단어가 무엇이었는지 모르게 한다는 뜻입니다. 그리고 인공 신경망에게 이렇게 마스킹 된 단어들을(Masked words) 예측하도록 합니다.\n",
    "<br />\n",
    "\n",
    "문장 중간에 구멍을 뚫어놓고, 구멍에 들어갈 단어들을 예측하게 하는 식입니다. 우리가 영어 시험을 볼 때 종종 마주하는 빈칸 채우기 문제에 비유할 수 있습니다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "96512990021685504d4683198faad895f5cd0e4b7b1aa29365fef97d0a48eb34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
