{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subword Text Encoder는 텐서플로우를 통해 사용할 수 있는 서브워드 토크나이저입니다.\n",
    "\n",
    "BPE와 유사한 알고리즘인 WordPiece Model을 채택하였으며, 패키지를 통해 쉽게 단어들을 Subwords로 분리할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        My family and I normally do not watch local mo...\n",
       "1        Believe it or not, this was at one time the wo...\n",
       "2        After some internet surfing, I found the \"Home...\n",
       "3        One of the most unheralded great works of anim...\n",
       "4        It was the Sixties, and anyone with long hair ...\n",
       "                               ...                        \n",
       "49995    the people who came up with this are SICK AND ...\n",
       "49996    The script is so so laughable... this in turn,...\n",
       "49997    \"So there's this bride, you see, and she gets ...\n",
       "49998    Your mind will not be satisfied by this nobud...\n",
       "49999    The chaser's war on everything is a weekly sho...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "train_df = pd.read_csv('IMDb_Reviews.csv')\n",
    "train_df['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(train_df['review'], target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the_', ', ', '. ', 'a_', 'and_', 'of_', 'to_', 's_', 'is_', 'br', 'in_', 'I_', 'that_', 'this_', 'it_', ' /><', ' />', 'was_', 'The_', 't_', 'as_', 'with_', 'for_', '.<', 'on_', 'but_', 'movie_', 'are_', ' (', 'have_']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.subwords[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty bad PRC cheapie which I rarely bother to watch over again, and it's no wonder -- it's slow and creaky and dull as a butter knife. Mad doctor George Zucco is at it again, turning a dimwitted farmhand in overalls (Glenn Strange) into a wolf-man. Unfortunately, the makeup is virtually non-existent, consisting only of a beard and dimestore fangs for the most part. If it were not for Zucco and Strange's presence, along with the cute Anne Nagel, this would be completely unwatchable. Strange, who would go on to play Frankenstein's monster for Unuiversal in two years, does a Lenny impression from \"Of Mice and Men\", it seems.<br /><br />*1/2 (of Four)\n"
     ]
    }
   ],
   "source": [
    "print(train_df['review'][20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sample question: [1590, 4162, 132, 7107, 1892, 2983, 578, 76, 12, 4632, 3422, 7, 160, 175, 372, 2, 5, 39, 8051, 8, 84, 2652, 497, 39, 8051, 8, 1374, 5, 3461, 2012, 48, 5, 2263, 21, 4, 2992, 127, 4729, 711, 3, 1391, 8044, 3557, 1277, 8102, 2154, 5681, 9, 42, 15, 372, 2, 3773, 4, 3502, 2308, 467, 4890, 1503, 11, 3347, 1419, 8127, 29, 5539, 98, 6099, 58, 94, 4, 1388, 4230, 8057, 213, 3, 1966, 2, 1, 6700, 8044, 9, 7069, 716, 8057, 6600, 2, 4102, 36, 78, 6, 4, 1865, 40, 5, 3502, 1043, 1645, 8044, 1000, 1813, 23, 1, 105, 1128, 3, 156, 15, 85, 33, 23, 8102, 2154, 5681, 5, 6099, 8051, 8, 7271, 1055, 2, 534, 22, 1, 3046, 5214, 810, 634, 8120, 2, 14, 71, 34, 436, 3311, 5447, 783, 3, 6099, 2, 46, 71, 193, 25, 7, 428, 2274, 2260, 6487, 8051, 8, 2149, 23, 1138, 4117, 6023, 163, 11, 148, 735, 2, 164, 4, 5277, 921, 3395, 1262, 37, 639, 1349, 349, 5, 2460, 328, 15, 5349, 8127, 24, 10, 16, 10, 17, 8054, 8061, 8059, 8062, 29, 6, 6607, 8126, 8053]\n",
      "\n",
      "Decoded String: Pretty bad PRC cheapie which I rarely bother to watch over again, and it's no wonder -- it's slow and creaky and dull as a butter knife. Mad doctor George Zucco is at it again, turning a dimwitted farmhand in overalls (Glenn Strange) into a wolf-man. Unfortunately, the makeup is virtually non-existent, consisting only of a beard and dimestore fangs for the most part. If it were not for Zucco and Strange's presence, along with the cute Anne Nagel, this would be completely unwatchable. Strange, who would go on to play Frankenstein's monster for Unuiversal in two years, does a Lenny impression from \"Of Mice and Men\", it seems.<br /><br />*1/2 (of Four)\n"
     ]
    }
   ],
   "source": [
    "tokenized_string = tokenizer.encode(train_df['review'][20])\n",
    "\n",
    "print('Tokenized sample question: {}'.format(tokenized_string))\n",
    "print()\n",
    "# 디코딩\n",
    "decoded_string = tokenizer.decode(tokenized_string)\n",
    "print('Decoded String: {}'.format(decoded_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8268"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어장 크기 확인\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1590 -------> Pre\n",
      "4162 -------> tty \n",
      "132 -------> bad \n",
      "7107 -------> PR\n",
      "1892 -------> C \n",
      "2983 -------> cheap\n",
      "578 -------> ie \n",
      "76 -------> which \n",
      "12 -------> I \n",
      "4632 -------> rarely \n",
      "3422 -------> bother \n",
      "7 -------> to \n",
      "160 -------> watch \n",
      "175 -------> over \n",
      "372 -------> again\n",
      "2 -------> , \n",
      "5 -------> and \n",
      "39 -------> it\n",
      "8051 -------> '\n",
      "8 -------> s \n",
      "84 -------> no \n",
      "2652 -------> wonder\n",
      "497 ------->  -- \n",
      "39 -------> it\n",
      "8051 -------> '\n",
      "8 -------> s \n",
      "1374 -------> slow \n",
      "5 -------> and \n",
      "3461 -------> cre\n",
      "2012 -------> ak\n",
      "48 -------> y \n",
      "5 -------> and \n",
      "2263 -------> dull \n",
      "21 -------> as \n",
      "4 -------> a \n",
      "2992 -------> butt\n",
      "127 -------> er \n",
      "4729 -------> kni\n",
      "711 -------> fe\n",
      "3 -------> . \n",
      "1391 -------> Mad\n",
      "8044 ------->  \n",
      "3557 -------> doctor \n",
      "1277 -------> George \n",
      "8102 -------> Z\n",
      "2154 -------> uc\n",
      "5681 -------> co \n",
      "9 -------> is \n",
      "42 -------> at \n",
      "15 -------> it \n",
      "372 -------> again\n",
      "2 -------> , \n",
      "3773 -------> turning \n",
      "4 -------> a \n",
      "3502 -------> dim\n",
      "2308 -------> wit\n",
      "467 -------> ted \n",
      "4890 -------> farm\n",
      "1503 -------> hand \n",
      "11 -------> in \n",
      "3347 -------> overa\n",
      "1419 -------> ll\n",
      "8127 -------> s\n",
      "29 ------->  (\n",
      "5539 -------> Glen\n",
      "98 -------> n \n",
      "6099 -------> Strange\n",
      "58 -------> ) \n",
      "94 -------> into \n",
      "4 -------> a \n",
      "1388 -------> wo\n",
      "4230 -------> lf\n",
      "8057 -------> -\n",
      "213 -------> man\n",
      "3 -------> . \n",
      "1966 -------> Unfortunately\n",
      "2 -------> , \n",
      "1 -------> the \n",
      "6700 -------> makeup\n",
      "8044 ------->  \n",
      "9 -------> is \n",
      "7069 -------> virtually \n",
      "716 -------> non\n",
      "8057 -------> -\n",
      "6600 -------> existent\n",
      "2 -------> , \n",
      "4102 -------> consist\n",
      "36 -------> ing \n",
      "78 -------> only \n",
      "6 -------> of \n",
      "4 -------> a \n",
      "1865 -------> bear\n",
      "40 -------> d \n",
      "5 -------> and \n",
      "3502 -------> dim\n",
      "1043 -------> est\n",
      "1645 -------> ore\n",
      "8044 ------->  \n",
      "1000 -------> fan\n",
      "1813 -------> gs \n",
      "23 -------> for \n",
      "1 -------> the \n",
      "105 -------> most \n",
      "1128 -------> part\n",
      "3 -------> . \n",
      "156 -------> If \n",
      "15 -------> it \n",
      "85 -------> were \n",
      "33 -------> not \n",
      "23 -------> for \n",
      "8102 -------> Z\n",
      "2154 -------> uc\n",
      "5681 -------> co \n",
      "5 -------> and \n",
      "6099 -------> Strange\n",
      "8051 -------> '\n",
      "8 -------> s \n",
      "7271 -------> prese\n",
      "1055 -------> nce\n",
      "2 -------> , \n",
      "534 -------> along \n",
      "22 -------> with \n",
      "1 -------> the \n",
      "3046 -------> cute \n",
      "5214 -------> Anne \n",
      "810 -------> Na\n",
      "634 -------> ge\n",
      "8120 -------> l\n",
      "2 -------> , \n",
      "14 -------> this \n",
      "71 -------> would \n",
      "34 -------> be \n",
      "436 -------> completely \n",
      "3311 -------> unw\n",
      "5447 -------> atch\n",
      "783 -------> able\n",
      "3 -------> . \n",
      "6099 -------> Strange\n",
      "2 -------> , \n",
      "46 -------> who \n",
      "71 -------> would \n",
      "193 -------> go \n",
      "25 -------> on \n",
      "7 -------> to \n",
      "428 -------> play \n",
      "2274 -------> Frank\n",
      "2260 -------> ens\n",
      "6487 -------> tein\n",
      "8051 -------> '\n",
      "8 -------> s \n",
      "2149 -------> monster \n",
      "23 -------> for \n",
      "1138 -------> Un\n",
      "4117 -------> ui\n",
      "6023 -------> versa\n",
      "163 -------> l \n",
      "11 -------> in \n",
      "148 -------> two \n",
      "735 -------> years\n",
      "2 -------> , \n",
      "164 -------> does \n",
      "4 -------> a \n",
      "5277 -------> Len\n",
      "921 -------> ny \n",
      "3395 -------> impression \n",
      "1262 -------> from\n",
      "37 ------->  \"\n",
      "639 -------> Of \n",
      "1349 -------> Mi\n",
      "349 -------> ce \n",
      "5 -------> and \n",
      "2460 -------> Men\n",
      "328 -------> \", \n",
      "15 -------> it \n",
      "5349 -------> seem\n",
      "8127 -------> s\n",
      "24 -------> .<\n",
      "10 -------> br\n",
      "16 ------->  /><\n",
      "10 -------> br\n",
      "17 ------->  />\n",
      "8054 -------> *\n",
      "8061 -------> 1\n",
      "8059 -------> /\n",
      "8062 -------> 2\n",
      "29 ------->  (\n",
      "6 -------> of \n",
      "6607 -------> Fou\n",
      "8126 -------> r\n",
      "8053 -------> )\n"
     ]
    }
   ],
   "source": [
    "for ts in tokenized_string:\n",
    "    print('{} -------> {}'.format(ts, tokenizer.decode([ts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Sentence : [137, 8051, 8, 910, 8057, 2169, 36, 7, 103, 13, 14, 32, 18, 7974, 8132, 8133, 997, 681, 8058]\n",
      "\n",
      "Decoded Sentence : It's mind-blowing to me that this film was evenxyz made.\n"
     ]
    }
   ],
   "source": [
    "# evenxyz로 토크나이저가 xyz를 어떻게 분리하는지 확인\n",
    "sample_string = \"It's mind-blowing to me that this film was evenxyz made.\"\n",
    "\n",
    "# encoding\n",
    "tokenized_string = tokenizer.encode(sample_string)\n",
    "print('Encoded Sentence : {}'.format(tokenized_string))\n",
    "print()\n",
    "\n",
    "# decoding\n",
    "decoded_string = tokenizer.decode(tokenized_string)\n",
    "print('Decoded Sentence : {}'.format(decoded_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137 ----> It\n",
      "8051 ----> '\n",
      "8 ----> s \n",
      "910 ----> mind\n",
      "8057 ----> -\n",
      "2169 ----> blow\n",
      "36 ----> ing \n",
      "7 ----> to \n",
      "103 ----> me \n",
      "13 ----> that \n",
      "14 ----> this \n",
      "32 ----> film \n",
      "18 ----> was \n",
      "7974 ----> even\n",
      "8132 ----> x\n",
      "8133 ----> y\n",
      "997 ----> z \n",
      "681 ----> made\n",
      "8058 ----> .\n"
     ]
    }
   ],
   "source": [
    "for ts in tokenized_string:\n",
    "  print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))\n",
    "\n",
    "  # xyz는 훈련 데이터에서 하나의 단어로 등장한 적이 없으므로 각각 분리"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네이버 영화 리뷰 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8112052</td>\n",
       "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8132799</td>\n",
       "      <td>디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4655635</td>\n",
       "      <td>폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9251303</td>\n",
       "      <td>와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10067386</td>\n",
       "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>8963373</td>\n",
       "      <td>포켓 몬스터 짜가 ㅡㅡ;;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>3302770</td>\n",
       "      <td>쓰.레.기</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>5458175</td>\n",
       "      <td>완전 사이코영화. 마지막은 더욱더 이 영화의질을 떨어트린다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>6908648</td>\n",
       "      <td>왜난 재미없었지 ㅠㅠ 라따뚜이 보고나서 스머프 봐서 그런가 ㅋㅋ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>8548411</td>\n",
       "      <td>포풍저그가나가신다영차영차영차</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                           document  label\n",
       "0        8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
       "1        8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n",
       "2        4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1\n",
       "3        9251303  와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...      1\n",
       "4       10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1\n",
       "...          ...                                                ...    ...\n",
       "199995   8963373                                     포켓 몬스터 짜가 ㅡㅡ;;      0\n",
       "199996   3302770                                              쓰.레.기      0\n",
       "199997   5458175                  완전 사이코영화. 마지막은 더욱더 이 영화의질을 떨어트린다.      0\n",
       "199998   6908648                왜난 재미없었지 ㅠㅠ 라따뚜이 보고나서 스머프 봐서 그런가 ㅋㅋ      0\n",
       "199999   8548411                                    포풍저그가나가신다영차영차영차      0\n",
       "\n",
       "[200000 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "train_data = pd.read_table('C:/Users/Myeong/dding/data/딥러닝-자연어처리입문/ratings.txt')\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "document    0\n",
       "label       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.dropna(how='any', inplace=True)\n",
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(train_data['document'], target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['. ',\n",
       " '..',\n",
       " '영화',\n",
       " '이_',\n",
       " '...',\n",
       " '의_',\n",
       " '는_',\n",
       " '다',\n",
       " '도_',\n",
       " ', ',\n",
       " '을_',\n",
       " '고_',\n",
       " '은_',\n",
       " '가_',\n",
       " '에_',\n",
       " '.. ',\n",
       " '한_',\n",
       " '너무_',\n",
       " '정말_',\n",
       " '를_',\n",
       " '고',\n",
       " '게_',\n",
       " '영화_',\n",
       " '지',\n",
       " '... ',\n",
       " '진짜_',\n",
       " '이',\n",
       " '다_',\n",
       " '요',\n",
       " '만_',\n",
       " '? ',\n",
       " '과_',\n",
       " '가',\n",
       " '로_',\n",
       " '지_',\n",
       " '나',\n",
       " '서_',\n",
       " '으로_',\n",
       " '아',\n",
       " '어',\n",
       " '....',\n",
       " '수_',\n",
       " '한',\n",
       " '와_',\n",
       " '도',\n",
       " '음',\n",
       " '네',\n",
       " '더_',\n",
       " '그냥_',\n",
       " '왜_']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.subwords[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오랜만에 본 제대로 된 범죄스릴러~\n"
     ]
    }
   ],
   "source": [
    "print(train_data['document'][20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sample question: [635, 90, 572, 208, 1781, 516, 8102]\n"
     ]
    }
   ],
   "source": [
    "print('Tokenized sample question: {}'.format(tokenizer.encode(train_data['document'][20])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 문장 : [635, 90, 572, 208, 1781, 516, 8102]\n",
      "\n",
      "기존 문장 : 오랜만에 본 제대로 된 범죄스릴러~\n"
     ]
    }
   ],
   "source": [
    "sample_string = train_data['document'][20]\n",
    "\n",
    "# 인코딩한 결과를 tokenized_string에 저장\n",
    "tokenized_string = tokenizer.encode(sample_string)\n",
    "print ('정수 인코딩 후의 문장 : {}'.format(tokenized_string))\n",
    "print()\n",
    "# 이를 다시 디코딩\n",
    "original_string = tokenizer.decode(tokenized_string)\n",
    "print ('기존 문장 : {}'.format(original_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635 ----> 오랜만에 \n",
      "90 ----> 본 \n",
      "572 ----> 제대로 \n",
      "208 ----> 된 \n",
      "1781 ----> 범죄\n",
      "516 ----> 스릴러\n",
      "8102 ----> ~\n"
     ]
    }
   ],
   "source": [
    "for ts in tokenized_string:\n",
    "  print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "96512990021685504d4683198faad895f5cd0e4b7b1aa29365fef97d0a48eb34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
